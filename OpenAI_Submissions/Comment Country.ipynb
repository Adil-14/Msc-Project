{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891f4460-2191-49fd-9602-abf9102a7d1d",
   "metadata": {},
   "source": [
    "# Comment Country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71216ff7-b621-485f-a3b9-e5ecc7fee3d9",
   "metadata": {},
   "source": [
    "- In this notebook, I prepared a batch to identify which country or region each thesis-relevant comment refers to.  \n",
    "- I used the rewritten comment text to build the batch JSONL file.  \n",
    "- The prompt asked GPT-4o to extract the name of the country or region mentioned in each comment, if any.  \n",
    "- To stay within the token limit, I split the batch into two parts before submitting it to OpenAI’s Batch API.  \n",
    "- Once the outputs were ready, I downloaded the JSONL files and extracted the `custom_id` and country response.  \n",
    "- I saved the results to a CSV, enabling further analysis of which countries are most frequently mentioned across the comments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73214ec-aa3b-4817-93ba-19aff01ae729",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469dcff-3315-4649-a9f0-98171004d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffcb30-153b-4a74-b76a-cd174f37ccc9",
   "metadata": {},
   "source": [
    "### Creating the Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596003e1-e4b4-485b-a1fd-486df8677c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/wkg5hpgj1kl1012mfw_qwby80000gn/T/ipykernel_2232/4066860633.py:6: DtypeWarning: Columns (11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"Final_Thesis_Merged.csv\")\n",
      "Building Gulf State Extraction Batch: 100%|█| 71441/71441 [00:07<00:00, 9851.77i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: batch_country_extraction.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading my Dataset\n",
    "df = pd.read_csv(\"Final_Thesis_Merged.csv\")\n",
    "\n",
    "# Output file name\n",
    "output_jsonl = \"batch_country_extraction.jsonl\"\n",
    "\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building Gulf State Extraction Batch\"):\n",
    "        comment_id = str(row[\"Comment_ID\"])\n",
    "        comment = str(row[\"Rewritten Comment\"])\n",
    "        event = str(row[\"Event\"])\n",
    "\n",
    "        task = {\n",
    "            \"custom_id\": comment_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are assisting a researcher analyzing Gulf-backed sports events.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f'''Below is a YouTube comment and the related sports event. Identify which Gulf state (e.g., Saudi Arabia, Qatar, UAE) is most associated with this comment. Use the event context to help you.\n",
    "\n",
    "Comment: \"{comment}\"\n",
    "\n",
    "Event: \"{event}\"\n",
    "\n",
    "If no Gulf state is clearly implied, respond with an empty string. Otherwise, respond with just one country name: Saudi Arabia, Qatar, or UAE.'''\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 10\n",
    "            }\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "print(f\"Saved: {output_jsonl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb6948-0970-4ddf-be08-e46e47fd2b07",
   "metadata": {},
   "source": [
    "### Splitting the Batch due to Token Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2228d1e4-7cfc-4d78-9020-b1d576307cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 35720 → batch_country_extraction_part1.jsonl\n",
      "Saved 35721 → batch_country_extraction_part2.jsonl\n"
     ]
    }
   ],
   "source": [
    "input_file = \"batch_country_extraction.jsonl\"  # Change to your file\n",
    "output_file_1 = input_file.replace(\".jsonl\", \"_part1.jsonl\")\n",
    "output_file_2 = input_file.replace(\".jsonl\", \"_part2.jsonl\")\n",
    "\n",
    "# Reading the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Splitting\n",
    "mid = len(lines) // 2\n",
    "part1 = lines[:mid]\n",
    "part2 = lines[mid:]\n",
    "\n",
    "# Writing to two new files\n",
    "with open(output_file_1, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(part1)\n",
    "\n",
    "with open(output_file_2, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(part2)\n",
    "\n",
    "print(f\"Saved {len(part1)} → {output_file_1}\")\n",
    "print(f\"Saved {len(part2)} → {output_file_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811cde0d-ffa2-4765-9e9f-e7dbae20511b",
   "metadata": {},
   "source": [
    "### Submitting Batches to Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "697c8d57-e9ec-4a31-80ce-3ecf9614fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submitting batch 1/2: batch_country_extraction_part1.jsonl\n",
      "Uploaded: File ID = file-8JUTCtPxpXYTk929LcpifC\n",
      "Batch submitted: Batch ID = batch_68163f99dbb8819095abba6692ef4663\n",
      "\n",
      "Submitting batch 2/2: batch_country_extraction_part2.jsonl\n",
      "Uploaded: File ID = file-PYuhXk4cesH9mvuEQFbYQh\n",
      "Batch submitted: Batch ID = batch_68163fa478a8819092324878bdb1b702\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"**************\"\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Your JSONL batch files (submitted one after the other with no wait)\n",
    "batch_files = [\n",
    "    \"batch_country_extraction_part1.jsonl\",\n",
    "    \"batch_country_extraction_part2.jsonl\"\n",
    "]\n",
    "\n",
    "# Submitting batches\n",
    "for i, file_path in enumerate(batch_files):\n",
    "    try:\n",
    "        print(f\"\\nSubmitting batch {i+1}/{len(batch_files)}: {file_path}\")\n",
    "\n",
    "        # Uploading file\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            upload = client.files.create(file=f, purpose=\"batch\")\n",
    "        print(f\"Uploaded: File ID = {upload.id}\")\n",
    "\n",
    "        # Creating batch job\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=upload.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(f\"Batch submitted: Batch ID = {batch.id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to submit {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f8371-8c44-417d-b034-9387e7df1506",
   "metadata": {},
   "source": [
    "### Downloading Json Output from Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd2f1560-678c-4fe8-baa3-857faaced50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: file-5gGugC2T7b6pfk56LfDCKY.jsonl\n",
      "File downloaded: file-8GHHh1dZMLRrW7jGLPuVo5.jsonl\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"*************\" \n",
    "\n",
    "# List of output file IDs from completed batches\n",
    "output_file_ids = [\n",
    "    \"file-5gGugC2T7b6pfk56LfDCKY\",\n",
    "    \"file-8GHHh1dZMLRrW7jGLPuVo5\"\n",
    "]\n",
    "\n",
    "# Downloading each output file\n",
    "for file_id in output_file_ids:\n",
    "    file_response = openai.files.content(file_id)\n",
    "\n",
    "    # Saving the file locally in binary mode\n",
    "    output_filename = f\"{file_id}.jsonl\"\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        for chunk in file_response.iter_bytes():\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"File downloaded: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda2bbde-02a0-40e9-8b84-0f3786f6fbca",
   "metadata": {},
   "source": [
    "### Converting Json Output to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b52a6324-7f61-421d-bf04-d5a96d6ce066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file-5gGugC2T7b6pfk56LfDCKY.jsonl\n",
      "Processing file-8GHHh1dZMLRrW7jGLPuVo5.jsonl\n",
      "\n",
      "Saved to: final_country_responses.csv\n",
      "Country\n",
      "Qatar           49848\n",
      "Saudi Arabia    16478\n",
      "UAE              3996\n",
      "                 1119\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "jsonl_files = [\n",
    "    \"file-5gGugC2T7b6pfk56LfDCKY.jsonl\",\n",
    "    \"file-8GHHh1dZMLRrW7jGLPuVo5.jsonl\"\n",
    "]\n",
    "\n",
    "# Valid Gulf states \n",
    "valid_countries = {\n",
    "    \"Saudi Arabia\",\n",
    "    \"Qatar\",\n",
    "    \"UAE\"\n",
    "}\n",
    "\n",
    "# Storing clean records\n",
    "records = []\n",
    "\n",
    "# Processing each file\n",
    "for jsonl_file in jsonl_files:\n",
    "    if not os.path.exists(jsonl_file):\n",
    "        print(f\"Missing file: {jsonl_file}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {jsonl_file}\")\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            comment_id = obj.get(\"custom_id\", \"\")\n",
    "            raw_response = obj.get(\"response\", {}).get(\"body\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "\n",
    "            # Clean response\n",
    "            country = raw_response.strip('`\"').strip()\n",
    "            if country not in valid_countries:\n",
    "                country = \"\"  # Default to empty string if not valid\n",
    "\n",
    "            records.append({\n",
    "                \"custom_id\": comment_id,\n",
    "                \"Country\": country\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(\"final_country_responses.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved to: final_country_responses.csv\")\n",
    "print(df[\"Country\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d915f6-8221-4f2c-a969-da7272b03323",
   "metadata": {},
   "source": [
    "### Merging Country Classfication in to Main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6755f861-fad5-465b-bd65-120691f7d08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/wkg5hpgj1kl1012mfw_qwby80000gn/T/ipykernel_818/1785757420.py:4: DtypeWarning: Columns (11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_final = pd.read_csv(\"Final_Thesis_Merged.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.read_csv(\"Final_Thesis_Merged.csv\")\n",
    "\n",
    "# Load the final country responses\n",
    "df_country = pd.read_csv(\"final_country_responses.csv\")\n",
    "\n",
    "# Merge based on Comment_ID and custom_id\n",
    "df_final = df_final.merge(\n",
    "    df_country,\n",
    "    how=\"left\",\n",
    "    left_on=\"Comment_ID\",\n",
    "    right_on=\"custom_id\"\n",
    ")\n",
    "\n",
    "# Dropping the redundant custom_id column\n",
    "df_final = df_final.drop(columns=[\"custom_id\"])\n",
    "\n",
    "# Overwriting the original thesis file\n",
    "df_final.to_csv(\"Final_Thesis_Merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a50f5-eddc-43ab-9aee-309e0a69be98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
