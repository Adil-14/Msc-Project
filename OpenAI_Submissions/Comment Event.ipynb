{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0c2d3c-601d-44e6-a191-d77e04192992",
   "metadata": {},
   "source": [
    "# Comment Event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da539ec0-51fd-44a3-b00a-c08f25dd150a",
   "metadata": {},
   "source": [
    "- In this notebook, I prepared a batch to identify which event each thesis-relevant comment refers to.  \n",
    "- I used the rewritten comment text to build the batch JSONL file.  \n",
    "- The prompt asked GPT-4o to match each comment to a specific event from my predefined list of sports-related events.  \n",
    "- To stay within the token limit, I split the batch into two parts before submitting it to OpenAI’s Batch API.  \n",
    "- Once the outputs were ready, I downloaded the JSONL files and extracted the `custom_id` and assigned event.  \n",
    "- I saved the results to a CSV for further analysis, enabling me to group and compare comments by the event they relate to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17662a-4b42-4fa1-bde7-df95890184d6",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ed6b5-8114-4269-9b45-f2dac0dffec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74542d95-ea17-415a-b573-42d833b1bc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71441 comments.\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "df = pd.read_csv(\"Thesis Relevant Comments.csv\")\n",
    "print(f\"Loaded {len(df)} comments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e84d14-7d68-4619-bc34-dff73474a2c7",
   "metadata": {},
   "source": [
    "### Creating the Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a3449d-a03c-4f1d-a6a7-7600cc718736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Event Class batch: 100%|██████| 71441/71441 [00:06<00:00, 10633.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: batch_event_class.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_jsonl = \"batch_event_class.jsonl\"\n",
    "\n",
    "# Opening the file and writing each comment as a separate task\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building Event Class batch\"):\n",
    "        comment_id = str(row[\"Comment_ID\"])\n",
    "        comment = row[\"Rewritten Comment\"]\n",
    "\n",
    "        # Building the request for GPT-4o Mini\n",
    "        task = {\n",
    "            \"custom_id\": comment_id,  \n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are assisting a researcher studying public opinion on major Gulf-backed sports events.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f'''Below is a YouTube comment. Your task is to identify which, if any, of the listed events the comment is referring to.\n",
    "\n",
    "Comment: \"{comment}\"\n",
    "\n",
    "Choose only ONE of the following events. If the comment does not relate to any, reply with an empty string:\n",
    "\n",
    "- Manchester City ownership\n",
    "- Paris Saint-Germain ownership\n",
    "- Newcastle United ownership\n",
    "- FIFA World Cup 2022\n",
    "- Formula 1\n",
    "- Saudi Pro League\n",
    "- LIV Golf\n",
    "- Gulf multi-sport hosting'''\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": 0,        \n",
    "                \"max_tokens\": 15         \n",
    "            }\n",
    "        }\n",
    "\n",
    "        \n",
    "        f.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "print(f\"Saved: {output_jsonl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdff095-3a82-4bb9-8efe-e5d035f41374",
   "metadata": {},
   "source": [
    "### Splitting the Batch due to Token Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a60b95-80a4-42f8-88fe-1b129a9a90fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 35720 → batch_event_class_part1.jsonl\n",
      "Saved 35721 → batch_event_class_part2.jsonl\n"
     ]
    }
   ],
   "source": [
    "input_file = \"batch_event_class.jsonl\"  \n",
    "output_file_1 = input_file.replace(\".jsonl\", \"_part1.jsonl\")\n",
    "output_file_2 = input_file.replace(\".jsonl\", \"_part2.jsonl\")\n",
    "\n",
    "# Reading the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Splitting\n",
    "mid = len(lines) // 2\n",
    "part1 = lines[:mid]\n",
    "part2 = lines[mid:]\n",
    "\n",
    "# Writing to two new files\n",
    "with open(output_file_1, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(part1)\n",
    "\n",
    "with open(output_file_2, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(part2)\n",
    "\n",
    "print(f\"Saved {len(part1)} → {output_file_1}\")\n",
    "print(f\"Saved {len(part2)} → {output_file_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52a5d1-9e75-4189-af40-5b48c85b9172",
   "metadata": {},
   "source": [
    "### Submitting Batches to Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059a72c4-bf50-41d8-b255-50abc9c3faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submitting batch 1/2: batch_event_class_part1.jsonl\n",
      "Uploaded: File ID = file-6uG14wTBtaYbsFz2nynrP3\n",
      "Batch submitted: Batch ID = batch_6812ac5ced848190a10dc8975757d0d2\n",
      "\n",
      "Submitting batch 2/2: batch_event_class_part2.jsonl\n",
      "Uploaded: File ID = file-HfzsW1zMnaAd9YXqsvsmoP\n",
      "Batch submitted: Batch ID = batch_6812ac689774819088143c10aa56601d\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"*****************\"\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# My JSONL batch files\n",
    "batch_files = [\n",
    "    \"batch_event_class_part1.jsonl\",\n",
    "    \"batch_event_class_part2.jsonl\"\n",
    "]\n",
    "\n",
    "# Submitting batches\n",
    "for i, file_path in enumerate(batch_files):\n",
    "    try:\n",
    "        print(f\"\\nSubmitting batch {i+1}/{len(batch_files)}: {file_path}\")\n",
    "\n",
    "        # Uploading file\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            upload = client.files.create(file=f, purpose=\"batch\")\n",
    "        print(f\"Uploaded: File ID = {upload.id}\")\n",
    "\n",
    "        # Creating batch job\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=upload.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(f\"Batch submitted: Batch ID = {batch.id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to submit {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef0415-156b-45c3-b730-78ed9927247f",
   "metadata": {},
   "source": [
    "### Downloading Json Output from Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec02f8e4-0ee5-4df5-8b09-69547f37ee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: file-92ujZ9WsuWPt51eniSUGX6.jsonl\n",
      "File downloaded: file-H65geZjE8zfizy5XjoYYnA.jsonl\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"***************\"  \n",
    "\n",
    "# List of output file IDs from completed batches\n",
    "output_file_ids = [\n",
    "    \"file-92ujZ9WsuWPt51eniSUGX6\",\n",
    "    \"file-H65geZjE8zfizy5XjoYYnA\"\n",
    "]\n",
    "\n",
    "# Downloading each output file\n",
    "for file_id in output_file_ids:\n",
    "    file_response = openai.files.content(file_id)\n",
    "\n",
    "    # Saving the file locally in binary mode\n",
    "    output_filename = f\"{file_id}.jsonl\"\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        for chunk in file_response.iter_bytes():\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"File downloaded: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9b01e-8b2d-4531-aa35-836570de98a4",
   "metadata": {},
   "source": [
    "### Converting Json Output to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2beb85a1-dd38-4d73-9c78-ed2ee5bf533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file-92ujZ9WsuWPt51eniSUGX6.jsonl\n",
      "Processing file-H65geZjE8zfizy5XjoYYnA.jsonl\n",
      "\n",
      "Saved to: final_event_responses.csv\n",
      "Event\n",
      "FIFA World Cup 2022              37554\n",
      "                                 17308\n",
      "Saudi Pro League                  5230\n",
      "Manchester City ownership         4194\n",
      "Newcastle United ownership        3298\n",
      "LIV Golf                          2295\n",
      "Gulf multi-sport hosting           706\n",
      "Formula 1                          476\n",
      "Paris Saint-Germain ownership      380\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "jsonl_files = [\n",
    "    \"file-92ujZ9WsuWPt51eniSUGX6.jsonl\",\n",
    "    \"file-H65geZjE8zfizy5XjoYYnA.jsonl\"\n",
    "]\n",
    "\n",
    "# Valid event options (from prompt)\n",
    "valid_events = {\n",
    "    \"Manchester City ownership\",\n",
    "    \"Paris Saint-Germain ownership\",\n",
    "    \"Newcastle United ownership\",\n",
    "    \"FIFA World Cup 2022\",\n",
    "    \"Formula 1\",\n",
    "    \"Saudi Pro League\",\n",
    "    \"LIV Golf\",\n",
    "    \"Gulf multi-sport hosting\"\n",
    "}\n",
    "\n",
    "# Storing clean records\n",
    "records = []\n",
    "\n",
    "# Processing each file\n",
    "for jsonl_file in jsonl_files:\n",
    "    if not os.path.exists(jsonl_file):\n",
    "        print(f\"Missing file: {jsonl_file}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {jsonl_file}\")\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            comment_id = obj.get(\"custom_id\", \"\")\n",
    "            raw_response = obj.get(\"response\", {}).get(\"body\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "\n",
    "            # Clean response\n",
    "            event = raw_response.strip('`\"').strip()\n",
    "            if event not in valid_events:\n",
    "                event = \"\"  # if not in valid set, default to empty string\n",
    "\n",
    "            records.append({\n",
    "                \"custom_id\": comment_id,\n",
    "                \"Event\": event\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(\"final_event_responses.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved to: final_event_responses.csv\")\n",
    "print(df[\"Event\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f4605-76ab-46aa-926e-08b19581a1bd",
   "metadata": {},
   "source": [
    "### Merging The Event Classes With the overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7181fd63-2ee5-4b87-a3fe-e480442be67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/wkg5hpgj1kl1012mfw_qwby80000gn/T/ipykernel_2012/4277356979.py:4: DtypeWarning: Columns (11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_final = pd.read_csv(\"Final_Thesis_Merged.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.read_csv(\"Final_Thesis_Merged.csv\")\n",
    "\n",
    "# Loading the final event responses\n",
    "df_events = pd.read_csv(\"final_event_responses.csv\")\n",
    "\n",
    "# Merging based on Comment_ID\n",
    "df_final = df_final.merge(\n",
    "    df_events,\n",
    "    how=\"left\",\n",
    "    left_on=\"Comment_ID\",\n",
    "    right_on=\"custom_id\"\n",
    ")\n",
    "\n",
    "# Dropping the extra custom_id column\n",
    "df_final = df_final.drop(columns=[\"custom_id\"])\n",
    "\n",
    "# Saving the result by overwriting the original file\n",
    "df_final.to_csv(\"Final_Thesis_Merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dfe412-312c-47c2-ae40-4b868594f527",
   "metadata": {},
   "source": [
    "### Filling Missing Event cells with Associated Trasncript Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2524afd4-525f-4447-b6e7-630c96e0724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/wkg5hpgj1kl1012mfw_qwby80000gn/T/ipykernel_2012/1736871357.py:1: DtypeWarning: Columns (11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_final = pd.read_csv(\"Final_Thesis_Merged.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.read_csv(\"Final_Thesis_Merged.csv\")\n",
    "\n",
    "# Loading the transcript dataset with Transcript_Event\n",
    "df_transcripts = pd.read_csv(\"Processed_YouTube_Transcripts.csv\")\n",
    "\n",
    "# Filling empty Event cells using matching Transcript_Event from Video_ID\n",
    "df_final[\"Event\"] = df_final.apply(\n",
    "    lambda row: df_transcripts[df_transcripts[\"Video_ID\"] == row[\"Video_ID\"]][\"Transcript_Event\"].values[0]\n",
    "    if pd.isna(row[\"Event\"]) and row[\"Video_ID\"] in df_transcripts[\"Video_ID\"].values else row[\"Event\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Overwriting the original file\n",
    "df_final.to_csv(\"Final_Thesis_Merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cfc00-7aa3-4e81-b745-a866b1e05b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
