{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f773c44c-07a5-4748-ac55-e14d84d10068",
   "metadata": {},
   "source": [
    "# Influenced by Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294623c7-4f14-4c1e-8874-a81a3b9e6650",
   "metadata": {},
   "source": [
    "- In this notebook, I prepared a batch to identify whether each thesis-relevant comment was influenced by the corresponding video transcript.  \n",
    "- I first used Sentence-BERT to generate sentence-level embeddings for all video transcripts.  \n",
    "- Each transcript was split into individual sentences, and embeddings were precomputed and stored by `Video_ID`.  \n",
    "- For each comment, I encoded it using SBERT and calculated cosine similarity between the comment and all transcript sentences from the same video.  \n",
    "- I selected the top 5 most similar transcript sentences for each comment and included them in the batch JSONL file.  \n",
    "- GPT-4o was then asked to return `1` if the comment appeared to be influenced by the transcript content, or `0` if it was not.  \n",
    "- The batch was split in two due to token limits and submitted via the OpenAI Batch API.  \n",
    "- Once the outputs were ready, I extracted the `custom_id` and influence label, and saved the results to a CSV for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d4d40-c837-4fdb-969b-c2f123fccf95",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7da3186d-c68f-45ac-a120-73b5bc6a1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b245ae7-fe10-4c24-9641-f093ff5a1858",
   "metadata": {},
   "source": [
    "### Generate Sentence Embeddings for Video Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67da205c-977f-4e18-940b-00080b10457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding transcripts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 162/162 [00:47<00:00, 3.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"Thesis Relevant Comments.csv\")\n",
    "df_transcripts = pd.read_csv(\"Processed_YouTube_Transcripts.csv\")\n",
    "\n",
    "# Load SBERT\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Precompute transcript embeddings\n",
    "transcript_embeddings = {}\n",
    "for _, row in tqdm(df_transcripts.iterrows(), total=len(df_transcripts), desc=\"ðŸ”„ Encoding transcripts\"):\n",
    "    video_id = row[\"Video_ID\"]\n",
    "    transcript_text = row[\"Transcript\"]\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', transcript_text)\n",
    "    if not sentences:\n",
    "        continue\n",
    "    transcript_embeddings[video_id] = (sentences, model.encode(sentences, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68332ae3-0da9-4853-904d-cee93950ecd9",
   "metadata": {},
   "source": [
    "### Identify Top 5 Transcript Sentences Closest to a Comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa39aa2-f5da-40a5-bf14-5358adf543bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 most similar transcript sentences for a given comment\n",
    "def get_top_similar_sentences(comment, video_id):\n",
    "    if video_id not in transcript_embeddings:\n",
    "        return [\"No transcript found\"] * 5\n",
    "    transcript_sentences, transcript_embedding = transcript_embeddings[video_id]\n",
    "    comment_embedding = model.encode(comment, convert_to_tensor=True)\n",
    "    similarities = util.pytorch_cos_sim(comment_embedding, transcript_embedding)[0]\n",
    "    top_indices = torch.topk(similarities, k=min(5, len(transcript_sentences))).indices\n",
    "    top_sentences = [transcript_sentences[i] for i in top_indices]\n",
    "    while len(top_sentences) < 5:\n",
    "        top_sentences.append(\"N/A\")\n",
    "    return top_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a981274-e15a-407e-92a9-5b6c21731bab",
   "metadata": {},
   "source": [
    "### Creating the Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b87b3c5d-26f4-4a1e-8cd0-198b4a5e5b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71441/71441 [36:50<00:00, 32.32it/s]\n",
      "Saved: batch_step_influenced_by_transcript.jsonl\n"
     ]
    }
   ],
   "source": [
    "output_jsonl = \"batch_step_influenced_by_transcript.jsonl\"\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating batch\"):\n",
    "        comment_id = str(row[\"Comment_ID\"])\n",
    "        rewritten_comment = row[\"Rewritten Comment\"]\n",
    "        video_id = row[\"Video_ID\"]\n",
    "        top_sentences = get_top_similar_sentences(rewritten_comment, video_id)\n",
    "\n",
    "        task = {\n",
    "            \"custom_id\": comment_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are helping a researcher determine whether a YouTube comment was influenced by a video transcript.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"\"\"Comment: \"{rewritten_comment}\"\n",
    "Transcript:\n",
    "- {top_sentences[0]}\n",
    "- {top_sentences[1]}\n",
    "- {top_sentences[2]}\n",
    "- {top_sentences[3]}\n",
    "- {top_sentences[4]}\n",
    "\n",
    "Was this comment influenced by the transcript? \n",
    "Reply ONLY with:\n",
    "1 = Yes, influenced\n",
    "0 = No, not influenced\"\"\"\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_tokens\": 5\n",
    "            }\n",
    "        }\n",
    "        f.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "print(f\"Saved: {output_jsonl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f69ccc-684d-4023-bbca-b6d900f4d403",
   "metadata": {},
   "source": [
    "### Splitting the Batch due to Token Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffed4e3f-f8b2-449d-8906-178ceb9e7e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 35720 â†’ batch_step_influenced_by_transcript_part1.jsonl\n",
      "Saved 35721 â†’ batch_step_influenced_by_transcript_part2.jsonl\n"
     ]
    }
   ],
   "source": [
    "# === Config ===\n",
    "input_file = \"batch_step_influenced_by_transcript.jsonl\"\n",
    "output_file_1 = input_file.replace(\".jsonl\", \"_part1.jsonl\")\n",
    "output_file_2 = input_file.replace(\".jsonl\", \"_part2.jsonl\")\n",
    "\n",
    "# === Read the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# === Split\n",
    "mid = len(lines) // 2\n",
    "part1 = lines[:mid]\n",
    "part2 = lines[mid:]\n",
    "\n",
    "# === Write to two new files\n",
    "with open(output_file_1, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(part1)\n",
    "\n",
    "with open(output_file_2, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(part2)\n",
    "\n",
    "# === Summary\n",
    "print(f\"Saved {len(part1)} â†’ {output_file_1}\")\n",
    "print(f\"Saved {len(part2)} â†’ {output_file_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e349da6-180a-4263-aab1-31ae21ce803b",
   "metadata": {},
   "source": [
    "### Submitting Batches to Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebe677c7-c33e-4a0b-b559-7fa75af6530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: batch_step_influenced_by_transcript_part1.jsonl -> File ID: file-RerJuxTQN1qAHHKgTUfsqD\n",
      "Submitted Batch 1 -> Batch ID: batch_67e083d901c48190a71da912bf7c840f\n",
      "Waiting 100 minutes before next submission...\n",
      "Uploaded: batch_step_influenced_by_transcript_part2.jsonl -> File ID: file-Mh8tajDU1pXwGN1zuRRueb\n",
      "Submitted Batch 2 -> Batch ID: batch_67e08af3bbf0819086e8b01ad3bbc097\n"
     ]
    }
   ],
   "source": [
    "# Set your OpenAI API key\n",
    "openai.api_key = \"*************\"\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# List of your batch files\n",
    "batch_files = [\n",
    "    \"batch_step_influenced_by_transcript_part1.jsonl\",\n",
    "    \"batch_step_influenced_by_transcript_part2.jsonl\"\n",
    "]\n",
    "\n",
    "# Submit each batch with 25-minute delay between them\n",
    "for i, file_path in enumerate(batch_files):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            upload = client.files.create(file=f, purpose=\"batch\")\n",
    "\n",
    "        print(f\"Uploaded: {file_path} -> File ID: {upload.id}\")\n",
    "\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=upload.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "\n",
    "        print(f\"Submitted Batch {i+1} -> Batch ID: {batch.id}\")\n",
    "\n",
    "        # Wait 100 minutes before submitting next batch (unless it's the last one)\n",
    "        if i < len(batch_files) - 1:\n",
    "            print(\"Waiting 100 minutes before next submission...\")\n",
    "            time.sleep(30 * 60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbe9c8-d8e1-4913-843f-41c8ff6a8467",
   "metadata": {},
   "source": [
    "### Downloading Json Output from Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28d0eb3d-1a5a-40bf-98f7-37c93dc2fc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: file-1CvpT2fRV1nTnGFCxEBThF.jsonl\n",
      "File downloaded: file-Y3EN2ZdZCNbALCJiV6vhDN.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Set OpenAI API Key\n",
    "openai.api_key = \"*****************\"  \n",
    "\n",
    "# List of output file IDs from completed batches\n",
    "output_file_ids = [\n",
    "    \"file-1CvpT2fRV1nTnGFCxEBThF\",  \n",
    "    \"file-Y3EN2ZdZCNbALCJiV6vhDN\",\n",
    "\n",
    "]\n",
    "\n",
    "# Download each output file properly\n",
    "for file_id in output_file_ids:\n",
    "    file_response = openai.files.content(file_id)\n",
    "\n",
    "    # Save the file locally in binary mode\n",
    "    output_filename = f\"{file_id}.jsonl\"\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        for chunk in file_response.iter_bytes():\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"File downloaded: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427fdf12-8cba-4ee6-bcc5-608ef3a6c5ff",
   "metadata": {},
   "source": [
    "### Converting Json Output to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bedc4d4b-0e6c-4c1e-94be-84dca78ff9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file-1CvpT2fRV1nTnGFCxEBThF.jsonl\n",
      "Processing file-Y3EN2ZdZCNbALCJiV6vhDN.jsonl\n",
      "\n",
      "Extracted responses saved to: influenced_by_transcript_responses.csv\n",
      "Influenced     : 47918\n",
      "Not Influenced : 23523\n"
     ]
    }
   ],
   "source": [
    "jsonl_files = [\n",
    "    \"file-1CvpT2fRV1nTnGFCxEBThF.jsonl\",\n",
    "    \"file-Y3EN2ZdZCNbALCJiV6vhDN.jsonl\"    \n",
    "]\n",
    "\n",
    "# Store extracted data\n",
    "extracted_data = []\n",
    "\n",
    "# Loop through files\n",
    "for jsonl_file in jsonl_files:\n",
    "    if not os.path.exists(jsonl_file):\n",
    "        print(f\" {jsonl_file} not found\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Processing {jsonl_file}\")\n",
    "\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            json_obj = json.loads(line)\n",
    "\n",
    "            custom_id = json_obj.get(\"custom_id\", \"\")\n",
    "            response = json_obj.get(\"response\", {}).get(\"body\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "\n",
    "            extracted_data.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"Influenced_by_Transcript\": response\n",
    "            })\n",
    "\n",
    "# Create and save CSV\n",
    "df = pd.DataFrame(extracted_data)\n",
    "output_csv = \"influenced_by_transcript_responses.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Summary\n",
    "influenced = (df[\"Influenced_by_Transcript\"] == \"1\").sum()\n",
    "not_influenced = (df[\"Influenced_by_Transcript\"] == \"0\").sum()\n",
    "\n",
    "print(f\"\\n Extracted responses saved to: {output_csv}\")\n",
    "print(f\" Influenced: {influenced}\")\n",
    "print(f\" Not Influenced: {not_influenced}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e2f57-5728-4451-ac4c-9affcad82e16",
   "metadata": {},
   "source": [
    "### Merging Transcript Influence Labels into Main Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce821929-8298-4a15-a299-b5c12fab71b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Merged CSV saved as Thesis_Relevant_With_Transcript_Influence.csv\n"
     ]
    }
   ],
   "source": [
    "df_thesis = pd.read_csv(\"Thesis Relevant Comments.csv\")\n",
    "df_influenced = pd.read_csv(\"influenced_by_transcript_responses.csv\")\n",
    "\n",
    "# If your thesis file has \"Comment_ID\" and the responses file has \"custom_id\", rename one:\n",
    "df_influenced = df_influenced.rename(columns={\"custom_id\": \"Comment_ID\"})\n",
    "\n",
    "# Merged on Comment_ID\n",
    "df_merged = pd.merge(df_thesis, df_influenced, on=\"Comment_ID\", how=\"left\")\n",
    "\n",
    "# Saved merged result\n",
    "df_merged.to_csv(\"Thesis_Relevant_With_Transcript_Influence.csv\", index=False)\n",
    "\n",
    "print(\" Merged CSV saved as Thesis_Relevant_With_Transcript_Influence.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e7ca6-706b-4ba2-afe5-db052a47828f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
