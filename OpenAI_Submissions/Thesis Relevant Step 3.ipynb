{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd853bb7-b89f-42c8-898a-462cf50478b5",
   "metadata": {},
   "source": [
    "## Thesis Relevant Comments Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbc4b0-b552-4f7f-bff0-ff4fea041c36",
   "metadata": {},
   "source": [
    "### Thesis Relevant Comments Step 3\n",
    "\n",
    "In this final filtering stage, all comments previously marked as **“DEFINITELY NOT”** were re-evaluated one more time — this time by comparing them directly with the corresponding **video transcript**. The goal was to catch any remaining comments that might have been wrongly excluded due to limited context or ambiguity in earlier steps.\n",
    "\n",
    "Each comment was re-checked to see if it **aligned with, reacted to, or indirectly referenced the transcript content**, especially in ways that could indicate relevance to sportswashing narratives.\n",
    "\n",
    "This extra step was added because a large number of comments had remained in the \"DEFINITELY NOT\" category after Step 2. By bringing the transcript into the comparison, borderline or subtle references that were previously missed could now be reconsidered. This helps ensure that no meaningful comment was lost, particularly those tied to themes only clear when seen in the context of what was said in the video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e89513-b447-424c-a0b9-b120e347506b",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b7853-ea05-4697-ac35-1e62c489c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ecb19-2645-4b51-b96d-e609374d39bc",
   "metadata": {},
   "source": [
    "### Loading comments & transcripts, initializing SBERT, and validating required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7617f38-a86e-4940-afaf-62eb69210e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded comments file: comments_definitely_not.csv (54177 rows)\n",
      "Loaded transcripts file: Processed_YouTube_Transcripts.csv (162 rows)\n",
      "Loading SBERT model...\n",
      "SBERT model loaded successfully.\n",
      "Column validation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Loading the CSV Files\n",
    "comments_csv = \"comments_definitely_not.csv\" \n",
    "transcript_csv = \"Processed_YouTube_Transcripts.csv\"\n",
    "\n",
    "df_comments = pd.read_csv(comments_csv)\n",
    "df_transcripts = pd.read_csv(transcript_csv)\n",
    "\n",
    "# Loading the SBERT Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Ensuring Required Columns Exist\n",
    "required_columns = [\"custom_id\", \"Rewritten Comment\", \"Video_ID\"]\n",
    "if not all(col in df_comments.columns for col in required_columns):\n",
    "    raise KeyError(f\" Missing required columns in comments CSV: {required_columns}\")\n",
    "\n",
    "if \"Video_ID\" not in df_transcripts.columns or \"Transcript\" not in df_transcripts.columns:\n",
    "    raise KeyError(\" Required columns ('Video_ID', 'Transcript') missing from transcript CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73864243-45ff-4e09-bf6e-0e07a895d9cc",
   "metadata": {},
   "source": [
    "### Compute SBERT embeddings & define top-5 transcript similarity helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "979922e7-d3a6-4743-ba7e-1673e1583c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding all comments in batch...\n",
      "Precomputing transcript embeddings...\n",
      "Processing Transcripts: 100%|█████████| 162/162 [00:47<00:00, 3.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Precomputing SBERT Embeddings for All Comments\n",
    "print(\" Encoding all comments in batch...\")\n",
    "comment_embeddings = model.encode(df_comments[\"Rewritten Comment\"].tolist(), convert_to_tensor=True)\n",
    "df_comments[\"comment_embedding\"] = list(comment_embeddings)\n",
    "\n",
    "# Precomputing Transcript Embeddings\n",
    "transcript_embeddings = {}\n",
    "print(\" Precomputing transcript embeddings...\")\n",
    "for _, row in tqdm(df_transcripts.iterrows(), total=len(df_transcripts), desc=\" Processing Transcripts\"):\n",
    "    video_id = row[\"Video_ID\"]\n",
    "    transcript_text = row[\"Transcript\"]\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', transcript_text)\n",
    "    \n",
    "    if len(sentences) == 0:\n",
    "        transcript_embeddings[video_id] = ([\"No meaningful sentences\"] * 5, torch.tensor([0.0]))\n",
    "    else:\n",
    "        transcript_embeddings[video_id] = (sentences, model.encode(sentences, convert_to_tensor=True))\n",
    "\n",
    "# Function to Get Top 5 Similar Sentences\n",
    "def get_top_similar_sentences(comment_embedding, video_id):\n",
    "    if video_id not in transcript_embeddings:\n",
    "        return [\"No transcript found\"] * 5, 0.0\n",
    "\n",
    "    transcript_sentences, transcript_embedding = transcript_embeddings[video_id]\n",
    "    similarities = util.pytorch_cos_sim(comment_embedding, transcript_embedding)[0]\n",
    "    max_similarity = torch.max(similarities).item()\n",
    "    \n",
    "    top_n = min(5, len(transcript_sentences))\n",
    "    top_indices = torch.topk(similarities, k=top_n).indices\n",
    "    top_sentences = [transcript_sentences[i] for i in top_indices]\n",
    "\n",
    "    while len(top_sentences) < 5:\n",
    "        top_sentences.append(\"N/A\")\n",
    "\n",
    "    return top_sentences, max_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf74f9b-1cb4-4aea-8470-98e413b8d29f",
   "metadata": {},
   "source": [
    "### Generate JSONL batch for transcript comparison classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b25f29d-e784-42e0-b760-9f2fc27c5291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing comments...\n",
      "Processing comments: 100%|█████████| 54177/54177 [00:27<00:00, 1955.10it/s]\n",
      "JSONL file created: batch_definitely_not_transcript_comparison.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Processing Comments\n",
    "output_jsonl = \"batch_definitely_not_transcript_comparison.jsonl\"\n",
    "print(\" Processing comments...\")\n",
    "\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in tqdm(df_comments.iterrows(), total=len(df_comments), desc=\" Processing comments\"):\n",
    "        comment_id = str(row[\"custom_id\"])\n",
    "        rewritten_comment = row[\"Rewritten Comment\"]\n",
    "        video_id = row[\"Video_ID\"]\n",
    "        comment_embedding = row[\"comment_embedding\"]\n",
    "\n",
    "        top_sentences, _ = get_top_similar_sentences(comment_embedding, video_id)\n",
    "\n",
    "        task = {\n",
    "            \"custom_id\": comment_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are analyzing YouTube comments by comparing them to their video transcripts. Your task is to determine whether the transcript provides meaningful context for the comment.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "**Comment:** \"{rewritten_comment}\"\n",
    "\n",
    "**Top 5 Transcript Sentences:**\n",
    "\"\"\" + \"\\n\".join([f\"- {sentence}\" for sentence in top_sentences]) + \"\"\"\n",
    "\n",
    "### Task:\n",
    "Determine whether this comment is relevant to discussions about sportswashing, human rights, financial ethics, corruption, or geopolitical motives.\n",
    "Additionally, if the comment makes a **positive or negative statement, opinion, or fact** about the **Middle East**, then it is considered relevant.\n",
    "\n",
    "### Instructions:\n",
    "- Respond **\"YES\"** if the comment relates to **any** of the above topics or expresses a **positive/negative statement about the Middle East**.\n",
    "- Respond **\"NO\"** if the comment is **only about match performance, players, goals, or unrelated topics**.\n",
    "\n",
    "### **Final Response Format:**  \n",
    "Respond **ONLY** with \"YES\" or \"NO\", nothing else.\n",
    "\"\"\"}\n",
    "                ],\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_tokens\": 5\n",
    "            }\n",
    "        }\n",
    "        f.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "print(f\" JSONL file created: {output_jsonl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698c6d9-e580-4168-9a7e-b15930a847ac",
   "metadata": {},
   "source": [
    "### The below script splits a large JSONL file into four smaller batches based on token count, using the GPT-4o Mini tokenizer to ensure each batch stays within a 30M token limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bfc0677-fe0c-49bf-ba47-0cc7c779b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting JSONL File: 54177it [05:15, 171.93it/s]\n",
      "Split Complete! JSONL files saved as:\n",
      "  - batch_definitely_not_part1.jsonl (≈30M tokens)\n",
      "  - batch_definitely_not_part2.jsonl (≈30M tokens)\n",
      "  - batch_definitely_not_part3.jsonl (≈30M tokens)\n",
      "  - batch_definitely_not_part4.jsonl (remaining tokens)\n"
     ]
    }
   ],
   "source": [
    "# Loading GPT-4o Mini Tokenizer\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "# Input and outputs\n",
    "input_jsonl = \"batch_definitely_not_transcript_comparison.jsonl\"\n",
    "outputs = [\n",
    "    \"batch_definitely_not_part1.jsonl\",\n",
    "    \"batch_definitely_not_part2.jsonl\",\n",
    "    \"batch_definitely_not_part3.jsonl\",\n",
    "    \"batch_definitely_not_part4.jsonl\",\n",
    "]\n",
    "\n",
    "batch_threshold = 30_000_000  # Max tokens per batch\n",
    "\n",
    "with contextlib.ExitStack() as stack:\n",
    "    files = [stack.enter_context(open(p, \"w\", encoding=\"utf-8\")) for p in outputs]\n",
    "\n",
    "    current_tokens = 0\n",
    "    batch_idx = 0  # 0..3 (last file collects the remainder)\n",
    "\n",
    "    with open(input_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\" Splitting JSONL File\"):\n",
    "            obj = json.loads(line)\n",
    "            msgs = obj[\"body\"][\"messages\"]\n",
    "            text = \" \".join(m.get(\"content\", \"\") for m in msgs)\n",
    "            num_tokens = len(enc.encode(text))\n",
    "\n",
    "            # Move to next batch if threshold exceeded (for first three parts)\n",
    "            if batch_idx < 3 and current_tokens + num_tokens > batch_threshold:\n",
    "                batch_idx += 1\n",
    "                current_tokens = 0\n",
    "\n",
    "            files[batch_idx].write(json.dumps(obj) + \"\\n\")\n",
    "            if batch_idx < 3:\n",
    "                current_tokens += num_tokens\n",
    "            # (For part 4, we don't track tokens—everything else goes there)\n",
    "            \n",
    "# Printing Results\n",
    "print(f\" Split Complete! JSONL files saved as:\")\n",
    "print(f\"  - {outputs[0]} (≈30M tokens)\")\n",
    "print(f\"  - {outputs[1]} (≈30M tokens)\")\n",
    "print(f\"  - {outputs[2]} (≈30M tokens)\")\n",
    "print(f\"  - {outputs[3]} (remaining tokens)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363acea4-825e-44b9-9550-6faacdcbf51b",
   "metadata": {},
   "source": [
    "### This script uploads and submits each JSONL batch file to OpenAI one by one, with a 100-minute gap between each to avoid hitting limits. It also prints out the job ID for each successful submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f15909a1-d2ea-4b60-9c44-bd865c72a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting batch_definitely_not_part2.jsonl (1/3)...\n",
      "batch_definitely_not_part2.jsonl uploaded successfully. File ID: file-RVvhzzF3hiaKojvthQmbe3\n",
      "Batch 1 submitted successfully! Job ID: batch_67db5ead2574819088d468edd49c9468\n",
      "Waiting 100 minutes before submitting the next batch...\n",
      "\n",
      "Submitting batch_definitely_not_part3.jsonl (2/3)...\n",
      "batch_definitely_not_part3.jsonl uploaded successfully. File ID: file-37EFCGF3D6XLPLiMijRjGS\n",
      "Batch 2 submitted successfully! Job ID: batch_67db764c8d248190b73433d402649f08\n",
      "Waiting 100 minutes before submitting the next batch...\n",
      "\n",
      "Submitting batch_definitely_not_part4.jsonl (3/3)...\n",
      "batch_definitely_not_part4.jsonl uploaded successfully. File ID: file-Ujgz9QvxrBw4uS7tRuLMGH\n",
      "Batch 3 submitted successfully! Job ID: batch_67db8dec5a708190834743c0e1ca3d39\n",
      "\n",
      "All batch jobs submitted successfully!\n",
      "- batch_definitely_not_part2.jsonl → Job ID: batch_67db5ead2574819088d468edd49c9468\n",
      "- batch_definitely_not_part3.jsonl → Job ID: batch_67db764c8d248190b73433d402649f08\n",
      "- batch_definitely_not_part4.jsonl → Job ID: batch_67db8dec5a708190834743c0e1ca3d39\n"
     ]
    }
   ],
   "source": [
    "# Set OpenAI API Key\n",
    "openai.api_key = \"*************\" \n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# List of batch files\n",
    "batch_files = [\n",
    "    \"batch_definitely_not_part2.jsonl\",\n",
    "    \"batch_definitely_not_part3.jsonl\",\n",
    "    \"batch_definitely_not_part4.jsonl\"\n",
    "]\n",
    "\n",
    "# Dictionary to store batch job IDs\n",
    "batch_jobs = {}\n",
    "\n",
    "# Uploading & Submitting Each Batch\n",
    "for index, batch_file in enumerate(batch_files):\n",
    "    if not os.path.exists(batch_file):\n",
    "        print(f\" {batch_file} not found. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n Submitting {batch_file} ({index+1}/{len(batch_files)})...\")\n",
    "\n",
    "        # Uploading the batch file\n",
    "        with open(batch_file, \"rb\") as f:\n",
    "            batch_file_upload = client.files.create(file=f, purpose=\"batch\")\n",
    "\n",
    "        print(f\" {batch_file} uploaded successfully. File ID: {batch_file_upload.id}\")\n",
    "\n",
    "        # Submitting Batch Job to OpenAI\n",
    "        batch_job = client.batches.create(\n",
    "            input_file_id=batch_file_upload.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "\n",
    "        batch_jobs[batch_file] = batch_job.id\n",
    "        print(f\" Batch {index + 1} submitted successfully! Job ID: {batch_job.id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {batch_file}: {e}\")\n",
    "\n",
    "    # Waiting 1 hour before submitting the next batch (except the last one)\n",
    "    if index < len(batch_files) - 1:\n",
    "        delay_minutes = 100  # 1 hour\n",
    "        print(f\" Waiting {delay_minutes} minutes before submitting the next batch...\")\n",
    "        time.sleep(delay_minutes * 60)  \n",
    "\n",
    "# Print Submitted Batch Job IDs\n",
    "print(\"\\n All batch jobs submitted successfully!\")\n",
    "for batch_file, job_id in batch_jobs.items():\n",
    "    print(f\"- {batch_file} → Job ID: {job_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa8504-4855-4b43-8eb5-5b1a0db187e7",
   "metadata": {},
   "source": [
    "### This script downloads the output files from each completed batch job using their file IDs and saves them locally as JSONL files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cdea8c6-13c3-4212-901a-06162f3835fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: file-5rNapKE6rPeGWmmxW69e6U.jsonl\n",
      "File downloaded: file-FRkTq1iabGCB1LG3WFjzVR.jsonl\n",
      "File downloaded: file-3scCRp5igqJnNtLXteqHdF.jsonl\n",
      "File downloaded: file-2X8oVYoqTHpedfhZKwqsbd.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Set OpenAI API Key\n",
    "openai.api_key = \"************\" \n",
    "\n",
    "# List of output file IDs from completed batches\n",
    "output_file_ids = [\n",
    "    \"file-5rNapKE6rPeGWmmxW69e6U\",  \n",
    "    \"file-FRkTq1iabGCB1LG3WFjzVR\",\n",
    "    \"file-3scCRp5igqJnNtLXteqHdF\",\n",
    "    \"file-2X8oVYoqTHpedfhZKwqsbd\"\n",
    "]\n",
    "\n",
    "# Download each output file properly\n",
    "for file_id in output_file_ids:\n",
    "    file_response = openai.files.content(file_id)\n",
    "\n",
    "    # Save the file locally in binary mode\n",
    "    output_filename = f\"{file_id}.jsonl\"\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        for chunk in file_response.iter_bytes():\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\" File downloaded: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fca70e-f471-4d2b-ba27-05e6cc0d8cfd",
   "metadata": {},
   "source": [
    "### This script goes through the downloaded JSONL files, extracts each comment's custom ID and model response, and saves everything into a CSV. It also prints how many responses were labelled as YES or NO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9db17179-bbb5-4f6a-a123-b5267724bc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file–5rNapKE6rPeGWmmxW69e6U.jsonl...\n",
      "Processing file–FRkTq1iabGCB1LG3WFjzVR.jsonl...\n",
      "Processing file–3scCRp5igqJnNtLXteqHdF.jsonl...\n",
      "Processing file–2X8oVYoqTHpedfhZKwqsbd.jsonl...\n",
      "\n",
      "Extracted responses saved to extracted_yes_no_responses.csv\n",
      "Total 'YES' responses: 20409\n",
      "Total 'NO' responses: 33768\n"
     ]
    }
   ],
   "source": [
    "# List of JSONL files\n",
    "jsonl_files = [\n",
    "    \"file-5rNapKE6rPeGWmmxW69e6U.jsonl\",  \n",
    "    \"file-FRkTq1iabGCB1LG3WFjzVR.jsonl\",\n",
    "    \"file-3scCRp5igqJnNtLXteqHdF.jsonl\",\n",
    "    \"file-2X8oVYoqTHpedfhZKwqsbd.jsonl\"\n",
    "]\n",
    "\n",
    "# Initializing a list to store extracted data\n",
    "extracted_data = []\n",
    "\n",
    "# Processing each JSONL file\n",
    "for jsonl_file in jsonl_files:\n",
    "    if not os.path.exists(jsonl_file):\n",
    "        print(f\" {jsonl_file} not found. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\" Processing {jsonl_file}...\")\n",
    "\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            json_obj = json.loads(line)\n",
    "\n",
    "            # Extract custom_id and response\n",
    "            custom_id = json_obj.get(\"custom_id\", \"\")\n",
    "            response_content = json_obj.get(\"response\", {}).get(\"body\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "\n",
    "            # Store in list\n",
    "            extracted_data.append({\"custom_id\": custom_id, \"response\": response_content})\n",
    "\n",
    "# Converting to DataFrame\n",
    "df_extracted = pd.DataFrame(extracted_data)\n",
    "\n",
    "# Saving to CSV\n",
    "output_csv = \"extracted_yes_no_responses.csv\"\n",
    "df_extracted.to_csv(output_csv, index=False)\n",
    "\n",
    "# Printing summary\n",
    "yes_count = (df_extracted[\"response\"] == \"YES\").sum()\n",
    "no_count = (df_extracted[\"response\"] == \"NO\").sum()\n",
    "\n",
    "print(f\"\\n Extracted responses saved to {output_csv}\")\n",
    "print(f\" Total 'YES' responses: {yes_count}\")\n",
    "print(f\" Total 'NO' responses: {no_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed17deb-ae42-4bb9-90c5-6b88d0678709",
   "metadata": {},
   "source": [
    "### This script merges the extracted YES/NO responses back with the original comment metadata using `custom_id` and saves the full result to a new CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6116d6b-579a-451e-8ab2-75346dcb38dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as merged_definitely_not_with_responses.csv\n",
      "Total merged records: 54177\n"
     ]
    }
   ],
   "source": [
    "# Loading Extracted Responses CSV\n",
    "responses_csv = \"extracted_yes_no_responses.csv\"  \n",
    "df_responses = pd.read_csv(responses_csv)\n",
    "\n",
    "# Loading Original Comments Metadata CSV\n",
    "comments_csv = \"comments_definitely_not.csv\"  \n",
    "df_comments = pd.read_csv(comments_csv)\n",
    "\n",
    "# Mergeing on custom_id\n",
    "df_merged = df_responses.merge(df_comments, on=\"custom_id\", how=\"left\")\n",
    "\n",
    "# Saving Merged Data to New CSV\n",
    "output_csv = \"merged_definitely_not_with_responses.csv\"\n",
    "df_merged.to_csv(output_csv, index=False)\n",
    "\n",
    "# Printing Summary\n",
    "print(f\" Merged file saved as {output_csv}\")\n",
    "print(f\" Total merged records: {len(df_merged)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb30706-c261-434d-8b27-0081e60b382d",
   "metadata": {},
   "source": [
    "### This script filters out only the YES responses from the merged data, cleans up extra columns, reorders the layout, and saves the final result to a new CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c859fca7-9950-4388-8f46-2863f6c59b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted YES responses saved as filtered_yes_responses.csv\n",
      "Total 'YES' responses: 20409\n"
     ]
    }
   ],
   "source": [
    "# Loading Merged CSV File\n",
    "input_csv = \"merged_definitely_not_with_responses.csv\"  \n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Filtering Only YES Responses from response_x\n",
    "df_yes = df[df[\"response_x\"] == \"YES\"].copy()\n",
    "\n",
    "# Dropping response_y Column\n",
    "df_yes.drop(columns=[\"response_y\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Renaming response_x to response\n",
    "df_yes.rename(columns={\"response_x\": \"response\"}, inplace=True)\n",
    "\n",
    "# Moving custom_id and response to the End\n",
    "cols = [col for col in df_yes.columns if col not in [\"custom_id\", \"response\"]]  # Get all columns except these two\n",
    "df_yes = df_yes[cols + [\"custom_id\", \"response\"]]  # Reorder columns\n",
    "\n",
    "# Saving Updated CSV\n",
    "output_csv = \"filtered_yes_responses.csv\"\n",
    "df_yes.to_csv(output_csv, index=False)\n",
    "\n",
    "# Printing Summary\n",
    "print(f\" Extracted YES responses saved as {output_csv}\")\n",
    "print(f\" Total 'YES' responses: {len(df_yes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3941000-dbca-4812-8aef-318053cedbf1",
   "metadata": {},
   "source": [
    "### This script combines two YES response CSVs into one, checks for any duplicate `custom_id`s, and saves the merged result to a new file. If duplicates are found, it shows a sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d7679e5-049e-4c02-84c0-20de53d9fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as: merged_yes_comments.csv\n",
      "Total Rows: 71441\n",
      "Duplicate custom_id Count: 0\n"
     ]
    }
   ],
   "source": [
    "# Loading CSV Files\n",
    "file1 = \"filtered_yes_responses.csv\"\n",
    "file2 = \"final_yes_comments.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "# Merging\n",
    "df_merged = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Checking for Duplicates in the custom_id\n",
    "duplicate_custom_ids = df_merged[df_merged.duplicated(subset=[\"custom_id\"], keep=False)]\n",
    "\n",
    "# Saving Merged CSV\n",
    "output_csv = \"merged_yes_comments.csv\"\n",
    "df_merged.to_csv(output_csv, index=False)\n",
    "\n",
    "# Printing Summary\n",
    "print(f\" Merged CSV saved as: {output_csv}\")\n",
    "print(f\" Total Rows: {len(df_merged)}\")\n",
    "print(f\" Duplicate custom_id Count: {len(duplicate_custom_ids)}\")\n",
    "\n",
    "# Showing Duplicate custom_id`s if they exist\n",
    "if not duplicate_custom_ids.empty:\n",
    "    print(\"\\n Duplicate custom_id Sample:\")\n",
    "    print(duplicate_custom_ids.head(10))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dac34d-8572-404b-b4d8-1d91b368acc0",
   "metadata": {},
   "source": [
    "### This script removes extra columns we no longer need from the merged YES comments file and saves a cleaner version for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ce7cf41-4ffd-4fbc-a73d-b833960d80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved as: cleaned_merged_yes_comments.csv\n",
      "Final Columns: ['Video_ID', 'Video_Title', 'Video_Category_Type', 'Channel_Name', 'Comment_ID', 'Author', 'Date', 'Likes', 'Replies_Count', 'Rewritten Comment']\n",
      "Total Rows: 71441\n"
     ]
    }
   ],
   "source": [
    "# Loading the Merged CSV\n",
    "input_csv = \"merged_yes_comments.csv\"\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Dropping Unwanted Columns\n",
    "columns_to_remove = [\"response\", \"custom_id\", \"Source_File\", \"Data_Source\", \"Comment\"]\n",
    "df_cleaned = df.drop(columns=columns_to_remove, errors=\"ignore\") \n",
    "\n",
    "# Saving the Cleaned CSV\n",
    "output_csv = \"cleaned_merged_yes_comments.csv\"\n",
    "df_cleaned.to_csv(output_csv, index=False)\n",
    "\n",
    "# Printing Confirmation\n",
    "print(f\" Cleaned CSV saved as: {output_csv}\")\n",
    "print(f\" Final Columns: {df_cleaned.columns.tolist()}\")\n",
    "print(f\" Total Rows: {len(df_cleaned)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073b500-6600-41e4-93ea-8712ba58c5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
