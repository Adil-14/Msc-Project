{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a5c09b-be2a-4562-b36f-14057680fe51",
   "metadata": {},
   "source": [
    "# Transcript Claims Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73043806-66a1-41b0-ab9b-520a28e259d5",
   "metadata": {},
   "source": [
    "- In this notebook, I prepared a batch to extract check-worthy factual claims directly from full YouTube video transcripts.  \n",
    "- I used the complete transcript text from each video and built a batch JSONL file where each entry was tied to a specific `Video_ID`.  \n",
    "- The prompt instructed GPT-4o to extract any claims related to sportswashing, human rights, financial corruption, or Gulf investments in global sports.  \n",
    "- If no relevant claims were present, the model was asked to reply with \"None\".  \n",
    "- The batch was submitted to the OpenAI Batch API and saved for further filtering and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284de61-653b-44aa-b42a-bf5ca5b1e2b3",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd4dee5-8b40-47dd-88bd-c229e5638363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "import openai\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d9332f-6784-44b7-bcf0-c5fc61409d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 161 transcripts.\n"
     ]
    }
   ],
   "source": [
    "# Loading transcript data\n",
    "df = pd.read_csv(\"Processed_YouTube_Transcripts.csv\")\n",
    "df = df.dropna(subset=[\"Transcript\"]).drop_duplicates(subset=[\"Video_ID\"]).reset_index(drop=True)\n",
    "print(f\"Loaded {len(df)} transcripts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097f5b0-0dde-45eb-aff6-5fc8521ab8a4",
   "metadata": {},
   "source": [
    "### Creating the Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "991417fa-a766-4d33-a7e6-9861720f541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building full transcript batch: 100% |████████| 161/161 [00:00<00:00, 324.32it/s]\n",
      "\n",
      "Saved JSONL file: batch_transcript_claim_extraction_full_4o.jsonl\n",
      "Total prompts: 161\n"
     ]
    }
   ],
   "source": [
    "output_jsonl = \"batch_transcript_claim_extraction_full_4o.jsonl\"\n",
    "\n",
    "# Opening the file and writing each transcript as a separate task\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building full transcript batch\"):\n",
    "        video_id = str(row[\"Video_ID\"])\n",
    "        transcript = row[\"Transcript\"]\n",
    "\n",
    "        # Building the prompt using the transcript\n",
    "        user_prompt = f\"\"\"Extract any check-worthy factual claims from the following YouTube transcript.\n",
    "\n",
    "The focus is on sportswashing, human rights, financial corruption, and Gulf investments in global sports.\n",
    "\n",
    "Return the claims as a bullet-point list. If there are no relevant claims, reply with:\n",
    "\n",
    "None\n",
    "\n",
    "Transcript:\n",
    "\\\"\\\"\\\"{transcript}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "        # Creating the task for GPT-4o\n",
    "        task = {\n",
    "            \"custom_id\": video_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are helping a researcher extract check-worthy claims from YouTube video transcripts.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": user_prompt\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 750\n",
    "            }\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "print(f\"\\nSaved JSONL file: {output_jsonl}\")\n",
    "print(f\"Total prompts: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964ef62-66a3-489a-b343-e59f1218f4cf",
   "metadata": {},
   "source": [
    "### Submitting Batches to Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e97ad19e-6218-4c7d-9935-d665ad70fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submitting batch 1/1: batch_transcript_claim_extraction_full_4o.jsonl\n",
      "Uploaded: File ID = file-PNmRDxXbfAdedATd5a9WFm\n",
      "Batch submitted: Batch ID = batch_67f1b9d8a5248190b9a24b806aab5a46\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"************\" \n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# JSONL batch files \n",
    "batch_files = [\n",
    "    \"batch_transcript_claim_extraction_full_4o.jsonl\"\n",
    "]\n",
    "\n",
    "# Submitting batches\n",
    "for i, file_path in enumerate(batch_files):\n",
    "    try:\n",
    "        print(f\"\\nSubmitting batch {i+1}/{len(batch_files)}: {file_path}\")\n",
    "\n",
    "        # Uploading file\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            upload = client.files.create(file=f, purpose=\"batch\")\n",
    "        print(f\"Uploaded: File ID = {upload.id}\")\n",
    "\n",
    "        # Creating batch job\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=upload.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(f\"Batch submitted: Batch ID = {batch.id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to submit {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11767c-c949-4060-a416-e71a6e67c85e",
   "metadata": {},
   "source": [
    "### Downloading Json Output from Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30349a2e-6e94-46ed-8357-89a29645a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: file-9xJPc6ofA1aG2fjbtvtYW4.jsonl\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"****************\" \n",
    "\n",
    "# List of output file IDs from completed batches\n",
    "output_file_ids = [\n",
    "    \"file-9xJPc6ofA1aG2fjbtvtYW4\"\n",
    "]\n",
    "\n",
    "# Downloading each output file\n",
    "for file_id in output_file_ids:\n",
    "    file_response = openai.files.content(file_id)\n",
    "\n",
    "    # Saving the file locally in binary mode\n",
    "    output_filename = f\"{file_id}.jsonl\"\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        for chunk in file_response.iter_bytes():\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"File downloaded: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebf210d-dc42-4f2d-92f3-f43376d660fa",
   "metadata": {},
   "source": [
    "### Converting Json Output to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "084a025d-5ded-481e-9c2c-53e3584071ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting claims from batch result: 161it [00:00, 8922.64it/s]\n"
     ]
    }
   ],
   "source": [
    "jsonl_path = \"file-9xJPc6ofA1aG2fjbtvtYW4.jsonl\"\n",
    "\n",
    "# Parse each line and extract claims\n",
    "video_claims = {}\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Extracting claims from batch result\"):\n",
    "        entry = json.loads(line)\n",
    "        video_id = entry[\"custom_id\"]\n",
    "\n",
    "        # Try to extract model's output\n",
    "        try:\n",
    "            response_text = entry[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            response_text = \"None\"  # fallback if something went wrong\n",
    "\n",
    "        # Convert response to list of claims\n",
    "        if response_text.lower() == \"none\":\n",
    "            claims = []\n",
    "        else:\n",
    "            claims = [\n",
    "                line.strip(\"- \").strip()\n",
    "                for line in response_text.splitlines()\n",
    "                if line.strip().startswith(\"-\") or len(line.strip()) > 0\n",
    "            ]\n",
    "\n",
    "        video_claims[video_id] = claims\n",
    "\n",
    "# Convert to DataFrame\n",
    "claims_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Video_ID\": vid,\n",
    "        \"Extracted_Claims\": claims,\n",
    "        \"Claim_Present\": int(len(claims) > 0)\n",
    "    }\n",
    "    for vid, claims in video_claims.items()\n",
    "])\n",
    "\n",
    "claims_df.to_csv(\"Extracted_Transcript_Claims.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03b1a9-a800-4797-8219-d747e87e0516",
   "metadata": {},
   "source": [
    "### Cleaning and Validating Extracted Transcript Claims§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9dbaae85-4a9b-4e81-9089-94162a1d4d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and saved: 'Extracted_Transcript_Claims_FIXED.csv'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Extracted_Transcript_Claims.csv\")\n",
    "\n",
    "# Define the claim cleaning function\n",
    "def parse_and_clean_claims(claim_string):\n",
    "    if pd.isna(claim_string) or claim_string.strip() in [\"[]\", \"None\", \"\"]:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        raw = claim_string.strip(\"[]\").strip()\n",
    "        parts = raw.split(\"', '\")\n",
    "        cleaned = []\n",
    "        for part in parts:\n",
    "            claim = part.strip().strip(\"'\\\"\").strip(\". \")\n",
    "            if len(claim.split()) >= 5:\n",
    "                cleaned.append(claim)\n",
    "        return cleaned\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Clean the claims\n",
    "df[\"Extracted_Claims\"] = df[\"Extracted_Claims\"].apply(parse_and_clean_claims)\n",
    "df[\"Num_Claims\"] = df[\"Extracted_Claims\"].apply(len)\n",
    "\n",
    "# Keep only the final columns\n",
    "df = df[[\"Video_ID\", \"Extracted_Claims\", \"Num_Claims\"]]\n",
    "\n",
    "# Save the final cleaned file\n",
    "df.to_csv(\"Extracted_Transcript_Claims_FIXED.csv\", index=False)\n",
    "print(\"Cleaned and saved: 'Extracted_Transcript_Claims_FIXED.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c179151-4614-4815-8d07-685b2a1add19",
   "metadata": {},
   "source": [
    "### Sampling and Viewing Random Extracted Claims from Transcript Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc81e25f-721f-499c-81cf-36dbc26628f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ID: hPmf0oSGZtM\n",
      "Number of Claims: 3\n",
      "\n",
      " Extracted Claims:\n",
      "1. Manchester City is accused of failing to provide accurate financial information across nine seasons.', \"Manchester City is accused of breaking UEFA's Financial Fair Play rules across five seasons.\", \"Manchester City is accused of breaking the Premier League's profitability and sustainability rules across three seasons.\", 'The Premier League investigation into Manchester City has been ongoing for almost six years\n",
      "2. The case against Manchester City started due to a young Portuguese computer expert hacking into emails of clubs and agents across Europe, with the information ending up in the hands of a German newspaper\n",
      "3. The punishment for Manchester City, if found guilty, could be unlimited, including potential relegation, expulsion from the Premier League, or fines.', \"The hearing regarding Manchester City's case is being held in secret, with only lawyers allowed to attend.\", 'The hearing is expected to take about 10 weeks, with the verdict taking months to be delivered, and the appeal process could extend the timeline by another six months\n"
     ]
    }
   ],
   "source": [
    "# Loading the cleaned claims file\n",
    "df = pd.read_csv(\"Extracted_Transcript_Claims_FIXED.csv\")\n",
    "\n",
    "# Filtering to rows that actually have claims\n",
    "df_with_claims = df[df[\"Num_Claims\"] > 0].reset_index(drop=True)\n",
    "\n",
    "# Picking a random row\n",
    "random_row = df_with_claims.sample(1).iloc[0]\n",
    "\n",
    "# Extracting data\n",
    "video_id = random_row[\"Video_ID\"]\n",
    "claims = eval(random_row[\"Extracted_Claims\"])  # convert stringified list to Python list\n",
    "num_claims = len(claims)\n",
    "\n",
    "# Print results\n",
    "print(f\"Video ID: {video_id}\")\n",
    "print(f\"Number of Claims: {num_claims}\")\n",
    "print(\"\\n Extracted Claims:\")\n",
    "for i, claim in enumerate(claims, 1):\n",
    "    print(f\"{i}. {claim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bac0c-d124-4480-92af-263875e7aa18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
