{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f528c322-589d-43ba-bb5e-9bcf6f34b55f",
   "metadata": {},
   "source": [
    "# Comment Emotion Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd6b43-ba52-4992-a721-0fd57d432966",
   "metadata": {},
   "source": [
    "- In this notebook, I prepared a batch to classify the emotional tone expressed in each thesis-relevant comment.  \n",
    "- I used the rewritten comment text to build the batch JSONL file.  \n",
    "- The prompt asked GPT-4o Mini to assign one of six predefined emotion categories to each comment: anger, worry, sadness, joy, surprise, or neutral.  \n",
    "- To stay within the token limit, I split the batch into two parts before submitting it to OpenAI’s Batch API.  \n",
    "- Once the outputs were ready, I downloaded the JSONL files and extracted the `custom_id` and assigned emotion class.  \n",
    "- I saved the results to a CSV so I could analyse the emotional distribution of comments across different countries, events, and categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db81ba6-1bd0-4b27-b9c5-95d885273e72",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5b8ce-23fb-4faa-8020-9a92f3263b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a59bc84-b80e-469f-8787-45961abd45b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71441 comments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/wkg5hpgj1kl1012mfw_qwby80000gn/T/ipykernel_14223/2738057105.py:4: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"Thesis_Relevant_With_Transcript_Influence_and_Agreement.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Loading the full dataset\n",
    "df = pd.read_csv(\"Thesis_Relevant_With_Transcript_Influence_and_Agreement.csv\")\n",
    "print(f\"Loaded {len(df)} comments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859f984-109f-4306-8d44-f9096f60a142",
   "metadata": {},
   "source": [
    "### Creating the Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5763f83e-d00d-4e5a-9f10-9115411a6df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Emotion Class batch (Balanced): 100%|█| 71441/71441 [00:07<00:00, 10045.6]\n",
      "Saved: batch_step_emotion_class.jsonl\n"
     ]
    }
   ],
   "source": [
    "output_jsonl = \"batch_step_emotion_class.jsonl\"\n",
    "\n",
    "# Opening the file and writing each comment as a separate task\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building Emotion Class batch (Balanced)\"):\n",
    "        comment_id = str(row[\"Comment_ID\"])\n",
    "        comment = row[\"Rewritten Comment\"]\n",
    "\n",
    "        # Building the request for GPT-4o Mini\n",
    "        task = {\n",
    "            \"custom_id\": comment_id,  # Used to keep track of each comment\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are helping a researcher identify the dominant emotion expressed in YouTube comments.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"\"\"Comment: \"{comment}\"\\n\\nWhat is the dominant emotion expressed in this comment?\\n\\nReply ONLY with one of the following:\\n- anger\\n- sadness\\n- joy\\n- hope\\n- disapproval\\n- neutral\"\"\"\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": 0,       \n",
    "                \"max_tokens\": 10        \n",
    "            }\n",
    "            \n",
    "        f.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "print(f\"Saved: {output_jsonl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a8557-457e-4f91-b986-8357943eeb3d",
   "metadata": {},
   "source": [
    "### Splitting the Batch due to Token Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9ef7675-c06a-41c4-bb6c-bd47b1d8564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 35720 → batch_step_emotion_class_part1.jsonl\n",
      "Saved 35721 → batch_step_emotion_class_part2.jsonl\n"
     ]
    }
   ],
   "source": [
    "input_file = \"batch_step_emotion_class.jsonl\" \n",
    "output_file_1 = input_file.replace(\".jsonl\", \"_part1.jsonl\")\n",
    "output_file_2 = input_file.replace(\".jsonl\", \"_part2.jsonl\")\n",
    "\n",
    "# Reading the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Splitting\n",
    "mid = len(lines) // 2\n",
    "part1 = lines[:mid]\n",
    "part2 = lines[mid:]\n",
    "\n",
    "# Writing to two new files\n",
    "with open(output_file_1, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(part1)\n",
    "\n",
    "with open(output_file_2, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(part2)\n",
    "\n",
    "print(f\"Saved {len(part1)} → {output_file_1}\")\n",
    "print(f\"Saved {len(part2)} → {output_file_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea484dec-1248-4ef9-a069-8c8b9cc5e695",
   "metadata": {},
   "source": [
    "### Submitting Batches to Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27d8532d-5960-4f59-8d64-e67fc63b6f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting batch 1/2: batch_step_emotion_class_part1.jsonl\n",
      "Uploaded: File ID = file–2m4feMySRM9SVT5zjPUK2H\n",
      "Batch submitted: Batch ID = batch_67e5efce27e0819094178262f1c29003\n",
      "\n",
      "Submitting batch 2/2: batch_step_emotion_class_part2.jsonl\n",
      "Uploaded: File ID = file–Bz2tsEeGTvPLAHSAGPKC6w\n",
      "Batch submitted: Batch ID = batch_67e5efd6eb28819080a585f5df896cbf\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"***********\" \n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# JSONL batch files \n",
    "batch_files = [\n",
    "    \"batch_step_emotion_class_part1.jsonl\",\n",
    "    \"batch_step_emotion_class_part2.jsonl\"\n",
    "]\n",
    "\n",
    "# Submitting batches\n",
    "for i, file_path in enumerate(batch_files):\n",
    "    try:\n",
    "        print(f\"\\nSubmitting batch {i+1}/{len(batch_files)}: {file_path}\")\n",
    "\n",
    "        # Uploading file\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            upload = client.files.create(file=f, purpose=\"batch\")\n",
    "        print(f\"Uploaded: File ID = {upload.id}\")\n",
    "\n",
    "        # Creating batch job\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=upload.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(f\"Batch submitted: Batch ID = {batch.id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to submit {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e39aa3-4cbb-4d6b-8037-52e91306a86f",
   "metadata": {},
   "source": [
    "### Downloading Json Output from Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a9e604-fafa-40cb-903f-cdf34de88ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: file-9DcDFsT8JDcAPeztC5fkQE.jsonl\n",
      "File downloaded: file-8vr65VFC1CsBMrnyPjkato.jsonl\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"***************\"\n",
    "\n",
    "# List of output file IDs from completed batches\n",
    "output_file_ids = [\n",
    "    \"file-9DcDFsT8JDcAPeztC5fkQE\",\n",
    "    \"file-8vr65VFC1CsBMrnyPjkato\"\n",
    "]\n",
    "\n",
    "# Downloading each output file\n",
    "for file_id in output_file_ids:\n",
    "    file_response = openai.files.content(file_id)\n",
    "\n",
    "    # Saving the file locally in binary mode\n",
    "    output_filename = f\"{file_id}.jsonl\"\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        for chunk in file_response.iter_bytes():\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"File downloaded: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a17bc1-6015-42d6-8ab0-77028f50bd1a",
   "metadata": {},
   "source": [
    "### Converting Json Output to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06ba4937-376f-43e2-bff5-c158b67a5d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file-9DcDFsT8JDcAPeztC5fkQE.jsonl\n",
      "Processing file-8vr65VFC1CsBMrnyPjkato.jsonl\n",
      "\n",
      "Extracted responses saved to: emotion_class_responses.csv\n",
      "Emotion breakdown:\n",
      "Emotion_Class\n",
      "disapproval       32610\n",
      "anger             21761\n",
      "joy                6481\n",
      "neutral            5562\n",
      "sadness            3276\n",
      "hope               1643\n",
      "fear                 25\n",
      "concern              15\n",
      "gratitude             8\n",
      "confusion             6\n",
      "shock                 5\n",
      "respect               5\n",
      "worry                 4\n",
      "frustration           4\n",
      "pride                 4\n",
      "nostalgia             3\n",
      "surprise              2\n",
      "relief                2\n",
      "shame                 2\n",
      "bittersweet           2\n",
      "conflicted            2\n",
      "sympathy              2\n",
      "jealousy              2\n",
      "solidarity            1\n",
      "compassion            1\n",
      "mixed feelings        1\n",
      "guilt                 1\n",
      "doubt                 1\n",
      "sarcasm               1\n",
      "appreciation          1\n",
      "amusement             1\n",
      "alarm                 1\n",
      "trust                 1\n",
      "admiration            1\n",
      "disgust               1\n",
      "anxiety               1\n",
      "curiosity             1\n",
      "bitterness            1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "jsonl_files = [\n",
    "    \"file-9DcDFsT8JDcAPeztC5fkQE.jsonl\",\n",
    "    \"file-8vr65VFC1CsBMrnyPjkato.jsonl\"\n",
    "]\n",
    "\n",
    "# Storing extracted data\n",
    "extracted_data = []\n",
    "\n",
    "# Looping through files\n",
    "for jsonl_file in jsonl_files:\n",
    "    if not os.path.exists(jsonl_file):\n",
    "        print(f\"{jsonl_file} not found\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {jsonl_file}\")\n",
    "\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            json_obj = json.loads(line)\n",
    "\n",
    "            custom_id = json_obj.get(\"custom_id\", \"\")\n",
    "            response = json_obj.get(\"response\", {}).get(\"body\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "\n",
    "            extracted_data.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"Emotion_Class\": response\n",
    "            })\n",
    "\n",
    "# Creating and saving CSV\n",
    "df = pd.DataFrame(extracted_data)\n",
    "output_csv = \"emotion_class_responses.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\nExtracted responses saved to: {output_csv}\")\n",
    "print(\"Emotion breakdown:\")\n",
    "print(df[\"Emotion_Class\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "928a555e-088c-4e8f-a65b-fb6abae5822d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/wkg5hpgj1kl1012mfw_qwby80000gn/T/ipykernel_14223/4130770039.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_main = pd.read_csv(\"Thesis_Relevant_With_Transcript_Influence_and_Agreement.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: category_responses.csv\n",
      "Merging: sentiment_responses.csv\n",
      "Merging: factual_or_opinion_responses.csv\n",
      "Merging: claim_detection_responses.csv\n",
      "Merging: emotion_class_responses.csv\n",
      "Final merged CSV saved as: Final_Thesis_Merged_With_All_Columns.csv\n"
     ]
    }
   ],
   "source": [
    "# Loading the master data\n",
    "df_main = pd.read_csv(\"Thesis_Relevant_With_Transcript_Influence_and_Agreement.csv\")\n",
    "df_main.rename(columns={\"Comment_ID\": \"custom_id\"}, inplace=True)\n",
    "\n",
    "# CSVs to merge\n",
    "response_csvs = [\n",
    "    \"category_responses.csv\",\n",
    "    \"sentiment_responses.csv\",\n",
    "    \"factual_or_opinion_responses.csv\",\n",
    "    \"claim_detection_responses.csv\",\n",
    "    \"emotion_class_responses.csv\"\n",
    "]\n",
    "\n",
    "# Merge each CSV\n",
    "for file in response_csvs:\n",
    "    print(f\"Merging: {file}\")\n",
    "    df = pd.read_csv(file)\n",
    "    df_main = pd.merge(df_main, df, on=\"custom_id\", how=\"left\")\n",
    "\n",
    "# Rename back to Comment_ID\n",
    "df_main.rename(columns={\"custom_id\": \"Comment_ID\"}, inplace=True)\n",
    "\n",
    "# Fill missing values\n",
    "df_main.fillna({\n",
    "    \"Category\": \"Unknown\",\n",
    "    \"Sentiment\": \"0\",\n",
    "    \"Factual_or_Opinion\": \"0\",\n",
    "    \"Claim_Detection\": \"0\",\n",
    "    \"Emotion_Class\": \"neutral\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Reordering columns\n",
    "cols = list(df_main.columns)\n",
    "\n",
    "# Define final order by name\n",
    "final_order = []\n",
    "\n",
    "# Start with all non-GPT columns (except the ones we want to reorder later)\n",
    "for col in cols:\n",
    "    if col not in [\"Category\", \"Sentiment\", \"Emotion_Class\", \"Factual_or_Opinion\", \"Claim_Detection\"]:\n",
    "        final_order.append(col)\n",
    "\n",
    "# Add GPT columns in order\n",
    "final_order += [\n",
    "    \"Category\",\n",
    "    \"Sentiment\",\n",
    "    \"Emotion_Class\",\n",
    "    \"Factual_or_Opinion\",\n",
    "    \"Claim_Detection\"\n",
    "]\n",
    "\n",
    "# Apply column order\n",
    "df_main = df_main[final_order]\n",
    "\n",
    "df_main.to_csv(\"Final_Thesis_Merged_With_All_Columns.csv\", index=False)\n",
    "print(\"Final merged CSV saved as: Final_Thesis_Merged_With_All_Columns.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5549b-3a8c-4bf5-a4fc-6d4a62d795c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
