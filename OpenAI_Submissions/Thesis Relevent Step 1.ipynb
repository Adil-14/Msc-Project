{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52a31ee-e6ca-4c3f-9805-81f9904245c4",
   "metadata": {},
   "source": [
    "## Thesis Relevant Comments Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3557d-7d96-43f8-8f1d-8d502a1244af",
   "metadata": {},
   "source": [
    "After collecting all YouTube comments, the next step is to identify which ones are actually relevant to the topic of sportswashing.\n",
    "\n",
    "Here, “thesis-relevant” means comments that mention:\n",
    "- A Middle Eastern country or Gulf nation\n",
    "- Topics like human rights, corruption, ethics, finance, image laundering, or political influence\n",
    "- Any opinions or reflections related to ownership, investments, or motives behind the event\n",
    "\n",
    "This step filters out random, unrelated comments (e.g. general football talk or player praise), so we only keep what’s useful for analysing narratives around sportswashing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9e41e-49af-425f-9720-7456ccd744ff",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d347942-4fbb-4e43-9515-1d9a04a33a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb990e-6522-4e28-b109-7c0892c91950",
   "metadata": {},
   "source": [
    "### Creating Json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253b0cd2-d9ad-471e-9efb-dd7b3b571679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created batch: batch_1.jsonl\n",
      "Created batch: batch_2.jsonl\n",
      "Created batch: batch_3.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Loading the CSV File\n",
    "df_cleaned = pd.read_csv(\"All_YouTube_Comments_Cleaned.csv\")\n",
    "\n",
    "# splitting into 3 even batches\n",
    "num_batches = 3\n",
    "batch_size = len(df_cleaned) // num_batches  # Each batch gets an equal share of rows\n",
    "\n",
    "# Generating 3 Large Batches\n",
    "for batch_num in range(num_batches):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = start_idx + batch_size if batch_num < num_batches - 1 else len(df_cleaned)  \n",
    "    \n",
    "    batch_df = df_cleaned.iloc[start_idx:end_idx].copy()  \n",
    "\n",
    "    jsonl_filename = f\"batch_{batch_num + 1}.jsonl\"\n",
    "\n",
    "    # Write each batch to a .jsonl file (one line per API task)\n",
    "    with open(jsonl_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in batch_df.iterrows():\n",
    "            # Build a single task for OpenAI Batch API\n",
    "            task = {\n",
    "                \"custom_id\": str(row[\"Comment_ID\"]),  # Use unique comment ID to track response\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o-mini\",  # Using smaller model for cost-effective classification\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are analyzing YouTube comments for sportswashing discussions.\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                        **Definition of Sportswashing:**  \n",
    "                        - When sports are used to improve a country/entity’s reputation while hiding **human rights abuses, corruption, or political issues**.  \n",
    "                        - Example: **Gulf states** (Saudi Arabia, Qatar, UAE) investing in global sports, hosting events (FIFA, F1), or owning clubs (Man City, PSG, Newcastle).  \n",
    "\n",
    "                        **Classification Rules:**  \n",
    "                        - `YES`: If the comment **mentions** sportswashing, Gulf investments, political influence, corruption in sports, or makes a positive or negative comment about the Middle East or Gulf countries.  \n",
    "                        - `NO`: If the comment is **only about sports**, lacks relevance, or is off-topic.  \n",
    "\n",
    "                        **YouTube Comment:** \\\"{row['Rewritten Comment']}\\\"  \n",
    "\n",
    "                        **Respond ONLY with `YES` or `NO`, nothing else.**\n",
    "                        \"\"\"}\n",
    "                    ],\n",
    "                    \"temperature\": 0.0,  \n",
    "                    \"max_tokens\": 5      # Response should be only YES or NO\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(task) + \"\\n\")  # Writing each task to JSONL line\n",
    "\n",
    "    print(f\" Created batch: {jsonl_filename}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1432d52-34b2-486f-9e54-e450aa57ebf1",
   "metadata": {},
   "source": [
    "### Submitting Batches to Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad7a0cae-8a8c-4771-a869-1ad9655976f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_1.jsonl uploaded successfully. File ID: file-ABC123ExampleID1\n",
      "Batch 1 submitted successfully! Job ID: batch_1234567890abcdef12345678\n",
      "batch_2.jsonl uploaded successfully. File ID: file-DEF456ExampleID2\n",
      "Batch 2 submitted successfully! Job ID: batch_abcdef1234567890abcdef12\n",
      "batch_3.jsonl uploaded successfully. File ID: file-R2Kc7pDozfopNQsa4myZTU\n",
      "Batch 3 submitted successfully! Job ID: batch_67d5723ce4f881900996a1cf95bf82466\n"
     ]
    }
   ],
   "source": [
    "# Setting OpenAI API Key\n",
    "openai.api_key = \"**********\" \n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# List of batch files to upload and submit\n",
    "batch_files = [\"batch_1.jsonl\", \"batch_2.jsonl\", \"batch_3.jsonl\"]\n",
    "\n",
    "for batch_file in batch_files:\n",
    "    # Check if the batch file actually exists before continuing\n",
    "    if not os.path.exists(batch_file):\n",
    "        print(f\"{batch_file} not found\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Uploading the JSONL batch file to OpenAI for processing\n",
    "        with open(batch_file, \"rb\") as f:\n",
    "            batch_file_upload = client.files.create(file=f, purpose=\"batch\")\n",
    "\n",
    "        print(f\" {batch_file} uploaded successfully. File ID: {batch_file_upload.id}\")\n",
    "\n",
    "        # Submitting the uploaded file as a batch job\n",
    "        batch_job = client.batches.create(\n",
    "            input_file_id=batch_file_upload.id,\n",
    "            endpoint=\"/v1/chat/completions\",  \n",
    "            completion_window=\"24h\"           # Run within 24h window\n",
    "        )\n",
    "\n",
    "        print(f\" {batch_file} submitted successfully! Job ID: {batch_job.id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print any errors if something goes wrong\n",
    "        print(f\"Error processing {batch_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67141e-c80a-4e2d-b945-3e2a0070da85",
   "metadata": {},
   "source": [
    "### Downloading Json Output from Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c01a086d-8f48-4632-a7f5-2f44bccf8220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: file-3NBaftwiCcsW49yC2tZZBw.jsonl\n",
      "File downloaded: file-L15bWH8WQiybjLkbKazp7Z.jsonl\n",
      "File downloaded: file-51PqRxZLj1ZVpXXJ1yTmZZ.jsonl\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"*********\" \n",
    "\n",
    "# List of output file IDs from batches that have finished processing\n",
    "output_file_ids = [\n",
    "    \"file-3NBaftwiCcsW49yC2tZZBw\", \n",
    "    \"file-L15bWH8WQiybjLkbKazp7Z\",\n",
    "    \"file-51PqRxZLj1ZVpXXJ1yTmZZ\"\n",
    "]\n",
    "\n",
    "# Looping through each output file and downloading it\n",
    "for file_id in output_file_ids:\n",
    "    file_response = openai.files.content(file_id)  # Fetch the file content\n",
    "\n",
    "    # Saving the file locally with .jsonl extension\n",
    "    output_filename = f\"{file_id}.jsonl\"\n",
    "    with open(output_filename, \"wb\") as f:  \n",
    "        for chunk in file_response.iter_bytes(): \n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"File downloaded: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37730d86-141b-4bc2-bcbc-f12634029472",
   "metadata": {},
   "source": [
    "### Converting Json file to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0444375d-c64a-49ee-8f30-c26286ac22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted JSONL to CSV: batch_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Loading JSONL files\n",
    "input_files = [\n",
    "    \"file-3NBaftwiCcsW49yC2tZZBw.jsonl\",\n",
    "    \"file-L15bWH8WQiybjLkbKazp7Z.jsonl\",\n",
    "    \"file-51PqRxZLj1ZVpXXJ1yTmZZ.jsonl\"\n",
    "]\n",
    "output_csv = \"batch_results.csv\"\n",
    "\n",
    "# Reading JSONL and extract required fields from each line\n",
    "data = []\n",
    "for input_file in input_files:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            json_obj = json.loads(line)  \n",
    "\n",
    "            # Extract: custom_id, model response, and any error message\n",
    "            data.append({\n",
    "                \"custom_id\": json_obj.get(\"id\"),  # Unique comment ID from our original prompt\n",
    "                \"response\": json_obj.get(\"response\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\"),\n",
    "                \"error\": json_obj.get(\"error\", \"\")  # If the call failed, capture the error\n",
    "            })\n",
    "\n",
    "# Convert list of dicts to DataFrame and save as CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Converted JSONL to CSV: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b1024-d71f-4d92-948e-de9524772330",
   "metadata": {},
   "source": [
    "### Merge LLM YES/NO Results with YouTube Comments and Save Final CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11431fd1-184d-4cd3-9fa2-600e00722485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created: final_processed_comments_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "# Loading CSVs\n",
    "df_results = pd.read_csv(\"batch_results.csv\")\n",
    "df_comments = pd.read_csv(\"All_YouTube_Comments.csv\")\n",
    "\n",
    "# Extract YES/NO responses robustly\n",
    "def extract_yes_no(response):\n",
    "    for parser in (json.loads, ast.literal_eval):\n",
    "        try:\n",
    "            parsed = parser(response)\n",
    "            return parsed[\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception:\n",
    "            continue\n",
    "    return \"Parse Error\"\n",
    "\n",
    "# Applying extraction and merge with comment text\n",
    "df_results[\"Response (YES/NO)\"] = df_results[\"response\"].apply(extract_yes_no)\n",
    "df_final = df_results.merge(\n",
    "    df_comments[[\"Comment_ID\", \"Rewritten Comment\"]],\n",
    "    left_on=\"custom_id\",\n",
    "    right_on=\"Comment_ID\",\n",
    "    how=\"left\"\n",
    ")[[\"custom_id\", \"Comment_ID\", \"Rewritten Comment\", \"Response (YES/NO)\"]]\n",
    "\n",
    "# Exporting to CSV\n",
    "df_final.to_csv(\"final_processed_comments_fixed.csv\", index=False)\n",
    "print(\" CSV file created: final_processed_comments_fixed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d7489-dd94-40fd-8986-acbe485ef3d2",
   "metadata": {},
   "source": [
    "### Count of Yes/No Responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17b9aae6-1cc0-42d4-96b3-dbeae23f8022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'YES' responses: 24157\n",
      "Number of 'NO' responses: 84259\n"
     ]
    }
   ],
   "source": [
    "# List of JSONL files\n",
    "input_files = [\n",
    "    \"file-3NBaftwiCcsW49yC2tZZBw.jsonl\",\n",
    "    \"file-L15bWH8WQiybjLkbKazp7Z.jsonl\",\n",
    "    \"file-51PqRxZLj1ZVpXXJ1yTmZZ.jsonl\"\n",
    "]\n",
    "\n",
    "# Reading and combine all files into one DataFrame\n",
    "data = []\n",
    "for file_name in input_files:\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        data.extend([\n",
    "            {\n",
    "                \"custom_id\": obj.get(\"custom_id\", \"\"),\n",
    "                \"response\": obj.get(\"response\", {}).get(\"body\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "            }\n",
    "            for obj in map(json.loads, f)\n",
    "        ])\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Count YES vs NO\n",
    "print(f\"Number of 'YES' responses: {(df['response'] == 'YES').sum()}\")\n",
    "print(f\"Number of 'NO' responses: {(df['response'] == 'NO').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81f92f25-dcd1-4576-bba7-eebc7207d660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comments to YouTube_Comments_Yes.csv\n",
      "Saved comments to YouTube_Comments_No.csv\n"
     ]
    }
   ],
   "source": [
    "# Loading the merged YouTube comments file with responses\n",
    "input_csv = \"final_processed_comments_fixed.csv\"\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Ensuring the response column exists and has valid values\n",
    "df = df[df[\"response\"].isin([\"YES\", \"NO\"])]\n",
    "\n",
    "# Separate into two DataFrames\n",
    "df_yes = df[df[\"response\"] == \"YES\"]\n",
    "df_no = df[df[\"response\"] == \"NO\"]\n",
    "\n",
    "# Saving each DataFrame into separate CSV files\n",
    "yes_csv = \"YouTube_Comments_Yes.csv\"\n",
    "no_csv = \"YouTube_Comments_No.csv\"\n",
    "\n",
    "df_yes.to_csv(yes_csv, index=False)\n",
    "df_no.to_csv(no_csv, index=False)\n",
    "\n",
    "print(f\"Saved comments to {yes_csv}\")\n",
    "print(f\"Saved comments to {no_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba053162-92fc-4c07-878f-a76b3fbd826e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
