{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71d1b40-7c4e-45cf-9a68-f6567fe24895",
   "metadata": {},
   "source": [
    "# Claim Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446301c-3651-4998-ad93-3fc891256f3f",
   "metadata": {},
   "source": [
    "- In this notebook, I prepared a batch to check if each comment contains a valid, check-worthy claim.  \n",
    "- I loaded all thesis-relevant comments and built a batch JSONL file using the rewritten comment text.  \n",
    "- The prompt asked GPT-4o to reply with just `1` for yes (claim present) or `0` for no.  \n",
    "- Because of the token limit, I split the batch into two parts before submitting it to OpenAI’s Batch API.  \n",
    "- Once the outputs were ready, I downloaded the JSONL files and extracted the `custom_id` and claim response.  \n",
    "- I saved the extracted results to a CSV and counted how many comments were labelled as containing a claim (`1`) and how many were not (`0`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218f8a0-68de-4efc-b9df-539e04b87865",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed160041-4d50-4765-a0dc-50c97867b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f30f0b-9eca-44a3-a958-0d5ab85ddd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71441 comments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/wkg5hpgj1kl1012mfw_qwby80000gn/T/ipykernel_14104/3311419059.py:4: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"Thesis_Relevant_With_Transcript_Influence_and_Agreement.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Loading the full dataset\n",
    "df = pd.read_csv(\"Thesis_Relevant_With_Transcript_Influence_and_Agreement.csv\")\n",
    "print(f\"Loaded {len(df)} comments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4292ba8-57e5-496a-8444-636bfe5761c5",
   "metadata": {},
   "source": [
    "### Creating the Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44f19e0-f259-43fd-9af9-d1492b17b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Claim Detection batch (Binary 1 = yes, 0 = no): 100%|█| 71441/71441 [00:00<00:00]\n",
      "Saved: batch_step_claim_detection.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Setting the output file name for the JSONL batch\n",
    "output_jsonl = \"batch_step_claim_detection.jsonl\"\n",
    "\n",
    "# Opening the file and writing each row as a separate task\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building Claim Detection batch (Binary 1 = yes, 0 = no)\"):\n",
    "        comment_id = str(row[\"Comment_ID\"])\n",
    "        comment = row[\"Rewritten Comment\"]\n",
    "\n",
    "        # Creating the request payload for GPT-4o\n",
    "        task = {\n",
    "            \"custom_id\": comment_id,  # Track the comment by its ID\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are helping a researcher determine whether a YouTube comment contains a check-worthy factual claim.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"\"\"Comment: \"{comment}\"\\n\\nDoes this comment contain a verifiable factual claim?\\n\\nReply ONLY with:\\n1 = yes\\n0 = no\\n\\nReply with just the number: 1 or 0\"\"\"\n",
    "                    }\n",
    "                ],\n",
    "                \"temperature\": 0,  \n",
    "                \"max_tokens\": 5    \n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "        f.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "print(f\"Saved: {output_jsonl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42c211-9b3c-475f-9f3f-ef5f36839c4d",
   "metadata": {},
   "source": [
    "### Splitting Batch due to Token Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62154311-1e1e-4c3d-b00b-8267e1145431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 35720 → batch_step_claim_detection_part1.jsonl\n",
      "Saved 35721 → batch_step_claim_detection_part2.jsonl\n"
     ]
    }
   ],
   "source": [
    "input_file = \"batch_step_claim_detection.jsonl\" \n",
    "output_file_1 = input_file.replace(\".jsonl\", \"_part1.jsonl\")\n",
    "output_file_2 = input_file.replace(\".jsonl\", \"_part2.jsonl\")\n",
    "\n",
    "# Reading the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Splitting\n",
    "mid = len(lines) // 2\n",
    "part1 = lines[:mid]\n",
    "part2 = lines[mid:]\n",
    "\n",
    "# Writing to two new files\n",
    "with open(output_file_1, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(part1)\n",
    "\n",
    "with open(output_file_2, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(part2)\n",
    "\n",
    "print(f\"Saved {len(part1)} → {output_file_1}\")\n",
    "print(f\"Saved {len(part2)} → {output_file_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37c6f9-c37b-46fc-99f3-ee43d3d66ce5",
   "metadata": {},
   "source": [
    "### Submitting Batches to Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0954692a-503c-4473-bba1-7998388ddb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting batch 1/2: batch_step_claim_detection_part1.jsonl\n",
      "Uploaded: File ID = file-RRvpCRsPw7NQ4ZPYj3XjUv\n",
      "Batch submitted: Batch ID = batch_67e5ec8b173c8190bf268370938960a2\n",
      "\n",
      "Submitting batch 2/2: batch_step_claim_detection_part2.jsonl\n",
      "Uploaded: File ID = file-KoTFC4TusJ8admJgvKpZXg\n",
      "Batch submitted: Batch ID = batch_67e5ec9407dc8190a7b1932560c885a3\n"
     ]
    }
   ],
   "source": [
    "# Setting my Openai Key\n",
    "openai.api_key = \"************\"\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Your JSONL batch files (submitted one after the other with no wait)\n",
    "batch_files = [\n",
    "    \"batch_step_claim_detection_part1.jsonl\",\n",
    "    \"batch_step_claim_detection_part2.jsonl\"\n",
    "]\n",
    "\n",
    "# Submitting batches\n",
    "for i, file_path in enumerate(batch_files):\n",
    "    try:\n",
    "        print(f\"\\nSubmitting batch {i+1}/{len(batch_files)}: {file_path}\")\n",
    "\n",
    "        # Uploading file\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            upload = client.files.create(file=f, purpose=\"batch\")\n",
    "        print(f\"Uploaded: File ID = {upload.id}\")\n",
    "\n",
    "        # Creating batch job\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=upload.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(f\"Batch submitted: Batch ID = {batch.id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to submit {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9254546-3242-407b-a68d-39e03778c20d",
   "metadata": {},
   "source": [
    "### Downloading Json Output from Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d820cad-a787-4a32-a464-75dece6d306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: file-2LUjxme1c4bQxUwqQrseJU.jsonl\n",
      "File downloaded: file-LB5qVVZ8eacaUiAJyAwgd8.jsonl\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"*************\"\n",
    "\n",
    "# List of output file IDs from completed batches\n",
    "output_file_ids = [\n",
    "    \"file-2LUjxme1c4bQxUwqQrseJU\",\n",
    "    \"file-LB5qVVZ8eacaUiAJyAwgd8\"\n",
    "]\n",
    "\n",
    "# Downloading each output file properly\n",
    "for file_id in output_file_ids:\n",
    "    file_response = openai.files.content(file_id)\n",
    "\n",
    "    # Saving the file locally in binary mode\n",
    "    output_filename = f\"{file_id}.jsonl\"\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        for chunk in file_response.iter_bytes():\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"File downloaded: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b9b731-becc-43d9-85ba-3f81e50804de",
   "metadata": {},
   "source": [
    "### Converting Json Output to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7bfd71f-3db3-4805-a8ea-a95a6c8085ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file-2LUjxme1c4bQxUwqQrseJU.jsonl\n",
      "Processing file-LB5qVVZ8eacaUiAJyAwgd8.jsonl\n",
      "\n",
      "Extracted responses saved to: claim_detection_responses.csv\n",
      "Contains claim    : 26537\n",
      "No claim detected : 44904\n"
     ]
    }
   ],
   "source": [
    "jsonl_files = [\n",
    "    \"file-2LUjxme1c4bQxUwqQrseJU.jsonl\",\n",
    "    \"file-LB5qVVZ8eacaUiAJyAwgd8.jsonl\"\n",
    "]\n",
    "\n",
    "# Storing extracted data\n",
    "extracted_data = []\n",
    "\n",
    "# Looping through files\n",
    "for jsonl_file in jsonl_files:\n",
    "    if not os.path.exists(jsonl_file):\n",
    "        print(f\"{jsonl_file} not found\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {jsonl_file}\")\n",
    "\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            json_obj = json.loads(line)\n",
    "\n",
    "            custom_id = json_obj.get(\"custom_id\", \"\")\n",
    "            response = json_obj.get(\"response\", {}).get(\"body\", {}).get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "\n",
    "            extracted_data.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"Claim_Detection\": response\n",
    "            })\n",
    "\n",
    "# Creating and saving CSV\n",
    "df = pd.DataFrame(extracted_data)\n",
    "output_csv = \"claim_detection_responses.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "claim = (df[\"Claim_Detection\"] == \"1\").sum()\n",
    "no_claim = (df[\"Claim_Detection\"] == \"0\").sum()\n",
    "\n",
    "print(f\"\\nExtracted responses saved to: {output_csv}\")\n",
    "print(f\"Contains claim    : {claim}\")\n",
    "print(f\"No claim detected : {no_claim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d22f9a-13a0-4016-b625-6458f7c435bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
