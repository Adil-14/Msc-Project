{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643460f9-84c9-4825-b39f-8a6ad2999fd6",
   "metadata": {},
   "source": [
    "# Checking Comments that had a Claim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e2dbc-0510-4688-b9aa-9eb4d7b8ce13",
   "metadata": {},
   "source": [
    "## Google Fact Check API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3bc94-2620-4fd7-aa3e-1cf51a22e940",
   "metadata": {},
   "source": [
    "- Filtered all comments marked as check-worthy and kept the claim text, video ID, and category.\n",
    "- Added unique IDs and saved the file as `claims_comments.csv`.\n",
    "- Sampled 10,000 claims across categories without oversampling.\n",
    "- Sent each claim to the Google Fact Check API to see if it matched anything.\n",
    "- If a match was found, I saved the matched text, publisher, rating, review date, and URL.\n",
    "- Saved everything to `fact_check_results_10000.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd8b6a-6997-4e40-9fd5-ed645ebe2e70",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3d5ce-7222-459c-a0e9-c57f6666fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da91a0e0-2ebd-4079-bd72-c8cfd4f6efbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 'claims_comments.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and filter for check-worthy claims\n",
    "df = pd.read_csv(\"Final_Thesis_Merged.csv\")\n",
    "checkworthy_df = df[df[\"Claim_Detection\"] == 1].copy()\n",
    "\n",
    "# Create cleaned DataFrame with necessary columns\n",
    "checkworthy_df = checkworthy_df[[\"Video_ID\", \"Rewritten Comment\", \"Category\"]]\n",
    "checkworthy_df.columns = [\"Video_ID\", \"Claim_Text\", \"Category\"]\n",
    "\n",
    "# Add unique Claim_ID\n",
    "checkworthy_df.insert(0, \"Claim_ID\", [f\"CMT_{i:04d}\" for i in range(1, len(checkworthy_df) + 1)])\n",
    "\n",
    "# Save to CSV\n",
    "checkworthy_df.to_csv(\"claims_comments.csv\", index=False)\n",
    "print(\"Saved: 'claims_comments.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24362a43-fcea-4f1d-8efd-a079f826f3e6",
   "metadata": {},
   "source": [
    "### Sampling 10,000 Claims Across Categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63fc9340-5409-4afb-af4a-0ef92ee254d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: sampled_10000_claims.csv with 10011 claims\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"claims_comments.csv\")\n",
    "\n",
    "category_sample_sizes = {\n",
    "    \"Human Rights\": 3560,\n",
    "    \"Geopolitics\": 2630,\n",
    "    \"Financial Ethics\": 1245,\n",
    "    \"Other\": 1015,\n",
    "    \"Corruption\": 700,\n",
    "    \"Media Criticism\": 480,\n",
    "    \"Sportswashing\": 380,\n",
    "    \"Environmental Concerns\": 1 \n",
    "}\n",
    "\n",
    "# Sample from each category \n",
    "sampled_df = pd.concat([\n",
    "    df[df[\"Category\"] == cat].sample(n=min(size, len(df[df[\"Category\"] == cat])), random_state=42)\n",
    "    for cat, size in category_sample_sizes.items()\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "sampled_df.to_csv(\"sampled_10000_claims.csv\", index=False)\n",
    "print(f\"Saved: sampled_10000_claims.csv with {len(sampled_df)} claims\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44290a16-8963-4ce0-9082-956c4d70673d",
   "metadata": {},
   "source": [
    "### Querying Sampled Claims Using the Google Fact Check API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99657f29-33da-489d-a47a-889b7b4350ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: fact_check_results_10000.csv with 1 matched results.\n"
     ]
    }
   ],
   "source": [
    "# Load sampled claims\n",
    "df = pd.read_csv(\"sampled_10000_claims.csv\")\n",
    "\n",
    "API_KEY = \"****************\"\n",
    "FACT_CHECK_URL = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Loop through claims and submit to API\n",
    "for idx, row in df.iterrows():\n",
    "    claim_id = row[\"Claim_ID\"]\n",
    "    claim_text = row[\"Claim_Text\"]\n",
    "    category = row[\"Category\"]\n",
    "\n",
    "    params = {\n",
    "        \"query\": claim_text,\n",
    "        \"key\": API_KEY,\n",
    "        \"languageCode\": \"en\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(FACT_CHECK_URL, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            claim_reviews = data.get(\"claims\", [])\n",
    "\n",
    "            # If there are results, save them\n",
    "            for review in claim_reviews:\n",
    "                entry = {\n",
    "                    \"Claim_ID\": claim_id,\n",
    "                    \"Category\": category,\n",
    "                    \"Query_Claim\": claim_text,\n",
    "                    \"Matched_Text\": review.get(\"text\", \"\"),\n",
    "                    \"Claim_Publisher\": review.get(\"claimReview\", [{}])[0].get(\"publisher\", {}).get(\"name\", \"\"),\n",
    "                    \"Claim_Review_URL\": review.get(\"claimReview\", [{}])[0].get(\"url\", \"\"),\n",
    "                    \"Review_Rating\": review.get(\"claimReview\", [{}])[0].get(\"textualRating\", \"\"),\n",
    "                    \"Review_Date\": review.get(\"claimReview\", [{}])[0].get(\"reviewDate\", \"\")\n",
    "                }\n",
    "                results.append(entry)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} for Claim_ID {claim_id}\")\n",
    "        time.sleep(0.5)  # Throttle to stay within rate limits\n",
    "    except Exception as e:\n",
    "        print(f\"Error at Claim_ID {claim_id}: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "# Save results to CSV\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(\"fact_check_results_10000.csv\", index=False)\n",
    "print(f\"Saved: fact_check_results_10000.csv with {len(result_df)} matched results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e5ea8-310c-4a24-b3f5-1629b510552a",
   "metadata": {},
   "source": [
    "Out of the 10,000 check-worthy YouTube claims I sampled across categories like Human Rights, Corruption, and Geopolitics, only one matched something in Google’s Fact Check API. This shows how most of the stuff people are saying in YouTube comments isn’t being picked up by formal fact-checking sources. It highlights a pretty big gap — the kinds of claims that come up in everyday online discussions often aren’t being verified anywhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff68016-d289-45d9-b908-53a9e6d3bb3e",
   "metadata": {},
   "source": [
    "## AZURE BING SEARCH & GPT 4o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75342a-a7da-4881-869f-6c11995a81af",
   "metadata": {},
   "source": [
    "- Used Azure OpenAI’s GPT-4o with Bing grounding to fact-check claims from YouTube comments.\n",
    "- Sent one claim at a time in a structured prompt via the API.\n",
    "- Model searched the web in real-time and returned:\n",
    "  - A result code:\n",
    "    - `1 = Likely True`\n",
    "    - `0 = Unverifiable`\n",
    "    - `-1 = Likely False`\n",
    "    - `-2 = Opinion or Speculation`\n",
    "  - A short explanation (1–3 sentences).\n",
    "  - 1–3 sources (titles or URLs).\n",
    "- Parsed responses and saved them incrementally to a CSV file.\n",
    "- This gave me a labelled dataset for factuality based on live web content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a5819-e090-4d95-939b-2c40ba2fc2b1",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e495b59-57c2-44b1-9660-87a26c9e8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de95167-82ec-4612-b562-f140db5088d2",
   "metadata": {},
   "source": [
    "### Setting Up Azure OpenAI Credentials and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2784aa3-c301-42ce-a761-09b9dd30bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"****************\"\n",
    "ENDPOINT = \"https://************.openai.azure.com/\"\n",
    "DEPLOYMENT_NAME = \"gpt-4o\"\n",
    "API_VERSION = \"2024-02-15-preview\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": API_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d24b546-55ca-474e-99c0-aba0c0950f8e",
   "metadata": {},
   "source": [
    "### Claim Verification Pipeline Using Azure OpenAI and Bing Grounding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823985a-5594-42ac-bf35-bd411f8686fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████▊| 26434/26537 [22:06:05<01:22,  1.25it/s]"
     ]
    }
   ],
   "source": [
    "# File Paths\n",
    "INPUT_CSV = \"claims_comments.csv\"\n",
    "OUTPUT_CSV = \"claim_comments_Results.csv\"\n",
    "CLAIM_BATCH_SIZE = 1  \n",
    "\n",
    "# Load and Filter Data\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df = df.dropna(subset=[\"Claim_Text\"]).reset_index(drop=True)\n",
    "\n",
    "# Batch utility\n",
    "def create_batched_claims(df, batch_size):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        yield df.iloc[i:i + batch_size]\n",
    "\n",
    "# Prompt Builder\n",
    "def build_prompt(claims):\n",
    "    system_msg = (\n",
    "        \"You are a fact-checking assistant. You will be given 1 claim at a time.\\n\"\n",
    "        \"For each claim:\\n\"\n",
    "        \"- Search the web using Bing grounding.\\n\"\n",
    "        \"- Return a result using the following code:\\n\"\n",
    "        \"  1 = Likely True\\n\"\n",
    "        \"  0 = Unverifiable\\n\"\n",
    "        \"  -1 = Likely False\\n\"\n",
    "        \"  -2 = Opinion or Speculation\\n\"\n",
    "        \"- Provide a 1–3 sentence explanation.\\n\"\n",
    "        \"- List 1–3 sources: preferably article titles, and URLs if available.\\n\\n\"\n",
    "        \"Format your response exactly like this:\\n\"\n",
    "        \"Claim:\\nResult: [1 / 0 / -1 / -2]\\nExplanation: [short paragraph]\\nSources: [source 1, source 2, source 3]\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_msg}]\n",
    "    for _, row in claims.iterrows():\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Claim {row['Claim_ID']}: {row['Claim_Text']}\"})\n",
    "    return messages\n",
    "\n",
    "# Extract structured fields\n",
    "def extract_fields(claim_id, claim_text, response_text):\n",
    "    result_match = re.search(r\"Result:\\s*\\[?(-?[0-2])\\]?\", response_text)\n",
    "    explanation_match = re.search(r\"Explanation:\\s*(.+?)Sources:\", response_text, re.DOTALL)\n",
    "    sources_match = re.search(r\"Sources:\\s*(.+)\", response_text, re.DOTALL)\n",
    "\n",
    "    return {\n",
    "        \"Claim_ID\": claim_id,\n",
    "        \"Claim_Text\": claim_text,\n",
    "        \"Result\": result_match.group(1).strip() if result_match else \"\",\n",
    "        \"Explanation\": explanation_match.group(1).strip() if explanation_match else \"\",\n",
    "        \"Sources\": sources_match.group(1).strip() if sources_match else \"\"\n",
    "    }\n",
    "\n",
    "# Main Loop\n",
    "records = []\n",
    "url = f\"{ENDPOINT}openai/deployments/{DEPLOYMENT_NAME}/chat/completions?api-version={API_VERSION}\"\n",
    "\n",
    "for batch in tqdm(create_batched_claims(df, CLAIM_BATCH_SIZE), total=(len(df) // CLAIM_BATCH_SIZE) + 1):\n",
    "    messages = build_prompt(batch)\n",
    "\n",
    "    body = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 1500,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=HEADERS, json=body)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            reply = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "            claim_id = batch.iloc[0][\"Claim_ID\"]\n",
    "            claim_text = batch.iloc[0][\"Claim_Text\"]\n",
    "\n",
    "            record = extract_fields(claim_id, claim_text, reply)\n",
    "            records.append(record)\n",
    "\n",
    "            # Save to output CSV incrementally\n",
    "            pd.DataFrame([record]).to_csv(\n",
    "                OUTPUT_CSV,\n",
    "                mode='a',\n",
    "                index=False,\n",
    "                header=not os.path.exists(OUTPUT_CSV)\n",
    "            )\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code)\n",
    "            print(response.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", str(e))\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"Processing complete. Output saved to:\", OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77810f5-cb67-4055-9cec-4926f205d3d5",
   "metadata": {},
   "source": [
    "### Generating Category-Wise Breakdown of Claim Verification Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b553bd1-c2b2-430a-abbb-ba8c4172351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Category  Likely False  Likely True  Opinion or Speculation\n",
      "          Human Rights           893         4181                    3131\n",
      "           Geopolitics           877         1884                    3327\n",
      "      Financial Ethics           264         1079                    1511\n",
      "                 Other           271          861                    1101\n",
      "            Corruption           219          606                     743\n",
      "       Media Criticism           120          287                     595\n",
      "         Sportswashing            77          339                     508\n",
      "Environmental Concerns             0            1                       0\n",
      "\n",
      "              Category  Unverifiable\n",
      "          Human Rights           955\n",
      "           Geopolitics           799\n",
      "      Financial Ethics           449\n",
      "                 Other           412\n",
      "            Corruption           289\n",
      "       Media Criticism           243\n",
      "         Sportswashing            91\n",
      "Environmental Concerns             0\n"
     ]
    }
   ],
   "source": [
    "# Load and map results\n",
    "df = pd.read_csv(\"claims_comments_Results.csv\")\n",
    "df['Result'] = df['Result'].map({\n",
    "    1: 'Likely True',\n",
    "    0: 'Unverifiable',\n",
    "    -1: 'Likely False',\n",
    "    -2: 'Opinion or Speculation'\n",
    "})\n",
    "\n",
    "# Group and sort\n",
    "by_cat = df.groupby(['Category', 'Result']).size().unstack(fill_value=0)\n",
    "by_cat = by_cat.loc[by_cat.sum(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "print(by_cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0114e-4bff-4142-9d31-33273f88b7c0",
   "metadata": {},
   "source": [
    "### Merging Transcript Agreement Labels into Claim-Level Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74692f5e-9cd1-4fe7-8407-acc9df38ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/wkg5hpgj1kl1012mfw_qwby80000gn/T/ipykernel_2986/972291167.py:4: DtypeWarning: Columns (11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_df = pd.read_csv(\"Final_Thesis_Merged.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Loading the CSV files\n",
    "merged_df = pd.read_csv(\"Final_Thesis_Merged.csv\")\n",
    "claims_df = pd.read_csv(\"claims_comments_Results.csv\")\n",
    "\n",
    "# Merge Agreed_with_Transcript into the claims DataFrame\n",
    "claims_df = claims_df.merge(\n",
    "    merged_df[['Rewritten Comment', 'Agreed_with_Transcript']],\n",
    "    left_on='Claim_Text',\n",
    "    right_on='Rewritten Comment',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop redundant 'Rewritten Comment' column\n",
    "claims_df.drop(columns=['Rewritten Comment'], inplace=True)\n",
    "\n",
    "# Overwrite the original CSV with the updated data\n",
    "claims_df.to_csv(\"claims_comments_Results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc19c1-cfcd-4e7c-ad9e-cee143af3f20",
   "metadata": {},
   "source": [
    "### Comparing Claim Verdicts with How Much They Agree with the Transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "087e00ca-5013-4aa8-bcc3-20a983f665a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Verdict  Agreement  Count\n",
      "8   Likely False     Agrees    240\n",
      "6   Likely False  Disagrees   1082\n",
      "7   Likely False    Neutral   1404\n",
      "14   Likely True     Agrees   2307\n",
      "12   Likely True  Disagrees   1942\n",
      "13   Likely True    Neutral   5029\n",
      "5        Opinion     Agrees   1375\n",
      "3        Opinion  Disagrees   4577\n",
      "4        Opinion    Neutral   5368\n",
      "11  Unverifiable     Agrees    342\n",
      "9   Unverifiable  Disagrees    936\n",
      "10  Unverifiable    Neutral   1974\n",
      "2            NaN     Agrees     56\n",
      "17           NaN     Agrees      5\n",
      "0            NaN  Disagrees    133\n",
      "15           NaN  Disagrees     20\n",
      "1            NaN    Neutral    228\n",
      "16           NaN    Neutral     22\n"
     ]
    }
   ],
   "source": [
    "# Load your CSV file\n",
    "df = pd.read_csv(\"claims_comments_Results.csv\")\n",
    "\n",
    "# Keep only rows with Agreed_with_Transcript values of -1, 0, or 1\n",
    "df = df[df['Agreed_with_Transcript'].astype(str).isin(['-1', '0', '1'])]\n",
    "\n",
    "# Drop rows with missing values in Result column\n",
    "df = df.dropna(subset=['Result'])\n",
    "\n",
    "# Convert both columns to integers\n",
    "df['Agreed_with_Transcript'] = df['Agreed_with_Transcript'].astype(int)\n",
    "df['Result'] = df['Result'].astype(int)\n",
    "\n",
    "# Group by both columns\n",
    "grouped = df.groupby(['Result', 'Agreed_with_Transcript']).size().reset_index(name='Count')\n",
    "\n",
    "# Mapping dictionaries\n",
    "result_map = {\n",
    "    1: 'Likely True',\n",
    "   -1: 'Likely False',\n",
    "   -2: 'Opinion',\n",
    "    0: 'Unverifiable'\n",
    "}\n",
    "\n",
    "agreement_map = {\n",
    "   -1: 'Disagrees',\n",
    "    0: 'Neutral',\n",
    "    1: 'Agrees'\n",
    "}\n",
    "\n",
    "# Map to readable labels\n",
    "grouped['Verdict'] = grouped['Result'].map(result_map)\n",
    "grouped['Agreement'] = grouped['Agreed_with_Transcript'].map(agreement_map)\n",
    "\n",
    "# Final output\n",
    "cleaned = grouped[['Verdict', 'Agreement', 'Count']].sort_values(by=['Verdict', 'Agreement'])\n",
    "\n",
    "# Print result\n",
    "print(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95694dc-33ba-44d8-83de-005bdf9a1fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
