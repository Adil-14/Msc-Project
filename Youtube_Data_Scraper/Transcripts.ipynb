{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc5c67d-33df-4081-ac4f-15a41f1aa812",
   "metadata": {},
   "source": [
    "# Downloading and Preparing YouTube Transcripts  \n",
    "Transcripts were extracted directly from selected YouTube videos using an automated tool, then saved in structured text format for further cleaning and analysis.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcc92f-28ad-49b1-8bd5-242cb9d84cc0",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afa545-baf7-4502-8c38-c00836a99551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7117e97-527a-4960-8376-5382e5121df6",
   "metadata": {},
   "source": [
    "### Download YouTube Transcripts and Titles to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c351e17-e087-4773-84f3-968a0461eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded transcript for Why the Saudi Arabian Grand Prix is so controversial (DLgNW9lCAaU)\n",
      "✅ Downloaded transcript for Saudi Arabia Exploits F-1 Racing For Sportswashing (r4K8V8btCtY)\n",
      "✅ Downloaded transcript for Formula 1: Lewis Hamilton speaks out about LGBTQ+ rights in Saudi Arabia (kRhg1dPS4Gw)\n",
      "✅ Downloaded transcript for Stefano Domenicali responds to criticisms of F1 hosting races in Qatar & Saudi Arabia (Cj8wtEFNVog)\n",
      "✅ Downloaded transcript for Bahrain's Grand Prix Sparks Human Rights Protests (_CLZf58vzbc)\n",
      "✅ Downloaded transcript for Sir Lewis Hamilton confronts human rights issues as Formula One returns (dbVki3gPYZs)\n",
      "✅ Downloaded transcript for Bernie Ecclestone: F1 and human rights concerns (C9sH8AD4jys)\n",
      "✅ Downloaded transcript for Formula 1's Europe Problem (6zEf9o-tTpc)\n",
      "✅ Downloaded transcript for I FLEW to a F1 race in the MIDDLE EAST... (_tSI_JV5lZY)\n",
      "✅ Downloaded transcript for How the Abu Dhabi F1 Track was Built (ReDGjoFTn58)\n",
      "✅ Downloaded transcript for David Beckham's Qatar Stopover (yRy2FOpCvws)\n",
      "📂 New transcripts added to YouTube_Transcripts.csv\n"
     ]
    }
   ],
   "source": [
    "# Your YouTube API Key\n",
    "API_KEY = \"AIzaSyDQPspanwFpHigCg7JjzPEGvcFMl0STPW0\"\n",
    "\n",
    "# List of YouTube Video IDs\n",
    "VIDEO_IDS = [\n",
    "    \"DLgNW9lCAaU\", \"r4K8V8btCtY\", \"kRhg1dPS4Gw\", \"Cj8wtEFNVog\", \"_CLZf58vzbc\",\n",
    "    \"dbVki3gPYZs\", \"C9sH8AD4jys\", \"6zEf9o-tTpc\", \"_tSI_JV5lZY\", \"ReDGjoFTn58\",\n",
    "    \"yRy2FOpCvws\"\n",
    "]\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = \"YouTube_Transcripts.csv\"\n",
    "\n",
    "# Function to get video title\n",
    "def get_video_title(video_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/videos?part=snippet&id={video_id}&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"items\" in data and len(data[\"items\"]) > 0:\n",
    "            return data[\"items\"][0][\"snippet\"][\"title\"]\n",
    "    return \"Unknown Title\"\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    existing_df = pd.read_csv(OUTPUT_FILE)\n",
    "    existing_video_ids = existing_df[\"Video_ID\"].tolist()\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    existing_video_ids = []\n",
    "\n",
    "# List to store transcript data\n",
    "transcripts_data = []\n",
    "\n",
    "# Loop through each video ID\n",
    "for video_id in VIDEO_IDS:\n",
    "    if video_id in existing_video_ids:\n",
    "        print(f\" Transcript for {video_id} already exists. Skipping...\")\n",
    "        continue\n",
    "    try:\n",
    "        video_title = get_video_title(video_id)  # Fetch video title\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        transcript_text = \" \".join([entry[\"text\"] for entry in transcript])\n",
    "\n",
    "        # Append to list\n",
    "        transcripts_data.append({\n",
    "            \"Video_ID\": video_id,\n",
    "            \"Video_Title\": video_title,\n",
    "            \"Transcript\": transcript_text\n",
    "        })\n",
    "\n",
    "        print(f\"✅ Downloaded transcript for {video_title} ({video_id})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Could not fetch transcript for {video_id}: {e}\")\n",
    "\n",
    "# Convert to DataFrame and append to the existing CSV file\n",
    "new_df = pd.DataFrame(transcripts_data)\n",
    "if not new_df.empty:\n",
    "    if not existing_df.empty:\n",
    "        final_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        final_df = new_df\n",
    "    final_df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "    print(f\"📂 New transcripts added to {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\" No new transcripts were added.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc1158b-5f81-49b2-9e60-1d5c3a88e26d",
   "metadata": {},
   "source": [
    "### Set up imports, API key, and file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adceb3a8-ce5f-4026-9210-11252827452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Key\n",
    "OPENAI_API_KEY = \"************\"  \n",
    "\n",
    "# Input and output file paths\n",
    "INPUT_FILE = \"YouTube_Transcripts.csv\"\n",
    "OUTPUT_FILE = \"Processed_YouTube_Transcripts.csv\"\n",
    "\n",
    "# Expected columns in the input\n",
    "REQUIRED_COLUMNS = [\"Video_ID\", \"Video_Title\", \"Transcript\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4172c-8157-4606-b72f-2da893cd1b6f",
   "metadata": {},
   "source": [
    "### Check input file and set up output CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc842d17-9352-4416-ab25-6fc5f83be690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input file exists\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(f\" Input file '{INPUT_FILE}' not found!\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Validate required columns\n",
    "for col in REQUIRED_COLUMNS:\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\" Missing required column: {col} in CSV file!\")\n",
    "\n",
    "# Initialize output CSV if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_FILE):\n",
    "    pd.DataFrame(columns=[\n",
    "        \"Video_ID\", \"Video_Title\", \"Transcript\", \n",
    "        \"Summary\", \"Topic\", \"Sentiment_Score\", \"Misinformation_Flag\"\n",
    "    ]).to_csv(OUTPUT_FILE, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2d803-e0ea-41cd-a8bc-b73dbd31420b",
   "metadata": {},
   "source": [
    "### Define function to process transcript with GPT-4o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0fc2a3-4ed6-4999-bfcd-6e7025a64268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcript(transcript):\n",
    "    if pd.isna(transcript) or transcript.strip() == \"\":\n",
    "        return {\n",
    "            \"Summary\": \"No meaningful content\",\n",
    "            \"Topic\": \"Uncategorized\",\n",
    "            \"Sentiment_Score\": 0,\n",
    "            \"Misinformation_Flag\": \"Unknown\"\n",
    "        }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are analyzing a transcript for a research study on sportswashing and misinformation in global sports. Your tasks:\n",
    "\n",
    "    1. Summarize the transcript (≤ 600 words)\n",
    "    2. Classify it into one of:\n",
    "       - Sportswashing\n",
    "       - Human Rights Issues\n",
    "       - Geopolitical Influence\n",
    "       - Financial Ethics in Sports\n",
    "       - Misinformation in Sports\n",
    "       - Other\n",
    "    3. Assign a sentiment score:\n",
    "       - -1 = Negative (critical of Gulf investments, ethics, rights)\n",
    "       -  0 = Neutral (balanced or factual)\n",
    "       -  1 = Positive (supportive of Gulf events/investments)\n",
    "    4. Detect misinformation (Yes/No)\n",
    "\n",
    "    Response format:\n",
    "    Summary: <...>\n",
    "    Topic: <...>\n",
    "    Sentiment_Score: <-1/0/1>\n",
    "    Misinformation_Flag: <Yes/No>\n",
    "\n",
    "    Now process this transcript:\n",
    "    {transcript}\n",
    "    \"\"\"\n",
    "\n",
    "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are analyzing YouTube transcripts for a sportswashing research study.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            timeout=20\n",
    "        )\n",
    "\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        result_lines = response_text.split(\"\\n\")\n",
    "\n",
    "        summary, topic, sentiment_score, misinformation_flag = \"Error in processing\", \"Error in classification\", 0, \"Unknown\"\n",
    "\n",
    "        for line in result_lines:\n",
    "            if line.startswith(\"Summary:\"):\n",
    "                summary = line.replace(\"Summary:\", \"\").strip()\n",
    "            elif line.startswith(\"Topic:\"):\n",
    "                topic = line.replace(\"Topic:\", \"\").strip()\n",
    "            elif line.startswith(\"Sentiment_Score:\"):\n",
    "                val = line.replace(\"Sentiment_Score:\", \"\").strip()\n",
    "                sentiment_score = {\"-1\": -1, \"0\": 0, \"1\": 1}.get(val, 0)\n",
    "            elif line.startswith(\"Misinformation_Flag:\"):\n",
    "                misinformation_flag = line.replace(\"Misinformation_Flag:\", \"\").strip()\n",
    "\n",
    "        if not topic or topic.lower() in [\"\", \"none\", \"error in classification\"]:\n",
    "            print(f\" Empty topic detected, retrying classification...\")\n",
    "            topic = retry_topic_classification(transcript)\n",
    "\n",
    "        return {\n",
    "            \"Summary\": summary,\n",
    "            \"Topic\": topic,\n",
    "            \"Sentiment_Score\": sentiment_score,\n",
    "            \"Misinformation_Flag\": misinformation_flag\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting GPT-4o response: {e}\")\n",
    "        return {\n",
    "            \"Summary\": \"Error in processing\",\n",
    "            \"Topic\": \"Error in classification\",\n",
    "            \"Sentiment_Score\": 0,\n",
    "            \"Misinformation_Flag\": \"Unknown\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb4eee-c6c9-4ff3-a0c1-dc00e133ce98",
   "metadata": {},
   "source": [
    "### Add fallback if GPT doesn't return a topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a845b-ae74-4509-a00d-1786313d7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_topic_classification(transcript):\n",
    "    retry_prompt = f\"\"\"\n",
    "    Classify this transcript into one of the following categories:\n",
    "    - Sportswashing\n",
    "    - Human Rights Issues\n",
    "    - Geopolitical Influence\n",
    "    - Financial Ethics in Sports\n",
    "    - Misinformation in Sports\n",
    "    - Other\n",
    "\n",
    "    Response format:\n",
    "    Topic: <One of the six categories>\n",
    "\n",
    "    Transcript:\n",
    "    {transcript}\n",
    "    \"\"\"\n",
    "\n",
    "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are analyzing transcript topics for a research study.\"},\n",
    "                {\"role\": \"user\", \"content\": retry_prompt}\n",
    "            ],\n",
    "            timeout=10\n",
    "        )\n",
    "\n",
    "        topic_value = response.choices[0].message.content.replace(\"Topic:\", \"\").strip()\n",
    "        return topic_value if topic_value else \"Other\"\n",
    "    except Exception as e:\n",
    "        print(f\" Topic retry failed: {e}\")\n",
    "        return \"Other\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71453aa7-9e69-46f4-bcb4-bbf3eef23d42",
   "metadata": {},
   "source": [
    "### Loop through transcripts and save results to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8acebd2-b348-4cfe-8039-1f447304e725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Transcripts:  54%|█████████▋        | 87/162 [11:39<09:56,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Empty topic detected, retrying classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Transcripts:  59%|██████████▋       | 96/162 [13:10<10:14,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Empty topic detected, retrying classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Transcripts:  68%|███████████▌     | 110/162 [15:14<07:30,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Empty topic detected, retrying classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Transcripts:  75%|████████████▋    | 121/162 [16:38<04:59,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Empty topic detected, retrying classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Transcripts:  81%|█████████████▋   | 131/162 [17:49<03:54,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Empty topic detected, retrying classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Transcripts:  86%|██████████████▌  | 139/162 [18:40<02:55,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Empty topic detected, retrying classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Transcripts: 100%|█████████████████| 162/162 [21:01<00:00,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processing complete! New CSV file saved as: Processed_YouTube_Transcripts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Transcripts\"):\n",
    "    transcript = row[\"Transcript\"]\n",
    "\n",
    "    try:\n",
    "        processed_result = process_transcript(transcript)\n",
    "        new_row = {\n",
    "            \"Video_ID\": row[\"Video_ID\"],\n",
    "            \"Video_Title\": row[\"Video_Title\"],\n",
    "            \"Transcript\": transcript,\n",
    "            \"Summary\": processed_result[\"Summary\"],\n",
    "            \"Topic\": processed_result[\"Topic\"],\n",
    "            \"Sentiment_Score\": processed_result[\"Sentiment_Score\"],\n",
    "            \"Misinformation_Flag\": processed_result[\"Misinformation_Flag\"]\n",
    "        }\n",
    "\n",
    "        pd.DataFrame([new_row]).to_csv(OUTPUT_FILE, mode='a', header=False, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process transcript: {transcript[:50]}... | Error: {e}\", flush=True)\n",
    "\n",
    "print(f\"✅ Processing complete! New CSV file saved as: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f3595-22c7-4317-ab3c-cd756eb381d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
